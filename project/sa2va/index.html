<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="OMG-Seg">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sa2VA</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">
  <link rel="icon" href="../../images/github_16x16.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/bulma-carousel.min.js"></script>
  <script src="../static/js/bulma-slider.min.js"></script>
  <script src="../static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://lxtgh.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/lxtGH/Tube-Link">
            TubeLink
          </a>
           <a class="navbar-item" href="https://lxtgh.github.io/project/omg_seg/">
            OMG-Seg
          </a>
          <a class="navbar-item" href="https://lxtgh.github.io/project/omg_llava/">
            OMG-LLaVA
          </a>
          <a class="navbar-item" href="https://xushilin1.github.io/rap_sam/">
            RAP-SAM
          </a>
          <a class="navbar-item" href="https://github.com/lxtGH/Awesome-Segmentation-With-Transformer">
            Transformer Segmentation Survey
          </a>
          <a class="navbar-item" href="https://github.com/jianzongwu/Awesome-Open-Vocabulary">
            Open Vocabulary Learning Survey
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yuanhaobo.me/">Haobo Yuan</a><sup>1</sup>,</span>

             <span class="author-block">
              <a href="https://lxtgh.github.io/">Xiangtai Li</a><sup>2</sup>,</span>
             <span class="author-block">
              <a href="https://zhang-tao-whu.github.io/">Tao Zhang</a><sup>2,3</sup>,</span>
             <span class="author-block">
              <a href="https://speedinghzl.github.io/">Zilong Huang</a><sup>2</sup>,</span>
             <span class="author-block">
              <a href="https://xushilin1.github.io/">Shilin Xu</a><sup>4</sup>,</span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=FjoRmF4AAAAJ&hl=en">Shunping Ji</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=T4gqdPkAAAAJ&view_op=list_works&sortby=pubdate">Yunhai Tong</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="http://luqi.info/">Lu Qi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Merced</span> &nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Beytedance Seed</span>&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>WHU</span>&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>4</sup>PKU</span>&nbsp;&nbsp;&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2501.04001"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/magic-research/Sa2VA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- HuggingFace link -->
              <span class="link-block">
                    <a href="https://huggingface.co/ByteDance/Sa2VA-4B" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>ðŸ¤— HuggingFace</span>
                  </a>
                </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./src/Sa2VA_teaser.png" alt="empty">
        <p>
           Sa2VA is able to segment the referred object and understand the whole scene, and supports image conversation, video conversation, image referring segmentation, video referring segmentation, and grounded caption generation with single-shot instruction-tuning. Sa2VA achieves strong results on multiple images, video referring segmentation, and chat benchmarks compared with existing MLLMs, such as GLaMM and OMG-LLaVA.
        </p>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-1">Demos</h2>
        <div class="content has-text-justified">
          
          <div class="hero-body">
            <p>
              (Demo 1) Input Video (Source: La La Land, 2016):
            </p>
            <video poster autoplay loop muted playsinline>
                <source src="video/exp_1.mp4" type="video/mp4">
                Error: Your browser does not support the video tag.
            </video>
            
            <p>
              Instruction: "Please segment the girl wearing the yellow dress."
            </p>
          </div>

          <div class="hero-body">
            <p>
              (Demo 2) Input Video (Source: La La Land, 2016):
            </p>
            <video autoplay loop muted>
                <source src="video/exp_2.mp4" type="video/mp4">
                Error: Your browser does not support the video tag.
            </video>
            <p>
              Instruction: "Please segment the main character."
            </p>
          </div>
          
          <div class="hero-body">
            <p>
              (Demo 3) Input Video (Source: Internet):
            </p>
            <video autoplay loop muted>
                <source src="video/apt_exp_1_all.mp4" type="video/mp4">
                Error: Your browser does not support the video tag.
            </video>
            <p>
              Instruction: "Please segment the person wearing sun glasses."
            </p>
          </div>

          <div class="hero-body">
            <p>
              (Demo 4) Input Video (Source: Internet):
            </p>
            <video autoplay loop muted>
                <source src="video/apt_exp_2_all.mp4" type="video/mp4">
                Error: Your browser does not support the video tag.
            </video>
            <p>
              Instruction: "Please segment the singing girl."
            </p>
          </div>

          <div class="hero-body">
            <p>
              (Demo 5) Input Video (Source: Internet):
            </p>
            <video autoplay loop muted>
                <source src="video/baodao_exp_1_all.mp4" type="video/mp4">
                Error: Your browser does not support the video tag.
            </video>
            <p>
              Instruction: "Please segment the guy in the center."
            </p>
          </div>

          <div class="hero-body">
            <p>
              (Demo 6) Input Video (Source: The Godfather, 1972):
            </p>
            <video autoplay loop muted>
                <source src="video/gf_exp1.mp4" type="video/mp4">
                Error: Your browser does not support the video tag.
            </video>
            <p>
              Instruction: "What is the atmosphere of the scene?"
            </p>
            <p>
              Answer: "The scene has a dark and mysterious atmosphere, with the men dressed in suits and ties, and the dimly lit room."
          </div>

          <div class="hero-body">
            <p>
              (Demo 7) Input Video (Source: Internet):
            </p>
            <video autoplay loop muted>
                <source src="video/exp_baodao_1.mp4" type="video/mp4">
                Error: Your browser does not support the video tag.
            </video>
            <p>
              Instruction: "What are the guys doing in the video?"
            </p>
            <p>
              Answer: "The guys in the video are dancing together in a group. They are performing a choreographed routine, moving in sync with each other."
          </div>
      
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Short Summary</h2>
        <div class="content has-text-justified">
          <p>
           This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into a shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content.
          </p>
          <p>
           Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <section class="setting teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Scope Comparison</h2>
        <div class="content has-text-justified">
    <div class="hero-body">
        <img src="./src/Sa2VA_setting.png" alt="empty">
        <p>
           Comparison of capabilities of different representative models. Our method supports various tasks and modalities. Benefiting from these interactive features on video, Sa2VA can perform multiple promptable tasks in the video: Ref-VOS, Image/Video Chat, Visual prompt understanding.
        </p>
    </div>
  </div>
      </div>
      </div>
  </div>

</section>


    <section class="setting teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method: Sa2VA</h2>
        <div class="content has-text-justified">
    <div class="hero-body">
        <img src="./src/Sa2VA_method.png" alt="empty">
        <p>
         Figure: Our proposed Sa2VA model. The model first encodes the input texts, visual prompts, images, and videos into token embeddings. These tokens are then processed through a large language model (LLM). The output text tokens are used to generate the [SEG] token and associated language outputs. The SAM-2 decoder receives the image and video features from the SAM-2 encoder, along with the [SEG] token, to generate corresponding image and video masks.
        </p>
    </div>
  </div>
      </div>
      </div>
  </div>

</section>


<section class="setting teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment Results: Sa2VA</h2>
        <div class="content has-text-justified">
    <div class="hero-body">
        <img src="./src/sa2va_results.png" alt="empty">
        <p>
          Figure: Experiment results on various settings, including image/video referring segmentation benchmarks and image/video chat benchmarks. Sa2VA achieves stronger performance than existing methods.
        </p>
    </div>
  </div>
      </div>
      </div>
  </div>

</section>


<section class="setting teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Labeled Examples: Ref-SAM-V dataset</h2>
        <div class="content has-text-justified">
    <div class="hero-body">
        <img src="./src/Ref-SAM-v_example.png" alt="empty">
        <p>
          Figure: The samples of our Ref-SAV benchmark. Our proposed benchmark features multi-granularity, complex occlusion and reappearing, and both short and long-format text expressions.
        </p>
    </div>
  </div>
      </div>
      </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{sa2va,
  title={Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos},
  author={Yuan, Haobo and Li, Xiangtai and Zhang, Tao and Huang, Zilong and Xu, Shilin and Ji, Shunping and Tong, Yunhai and Qi, Lu and Feng, Jiashi and Yang, Ming-Hsuan},
  journal={arXiv},
  year={2025}
}</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks for the project template by  <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies Project</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
