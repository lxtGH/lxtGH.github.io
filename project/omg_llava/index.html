<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="OMG-Seg">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OMG-Seg</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">
  <link rel="icon" href="../../images/github_16x16.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/bulma-carousel.min.js"></script>
  <script src="../static/js/bulma-slider.min.js"></script>
  <script src="../static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://lxtgh.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/lxtGH/Tube-Link">
            TubeLink
          </a>
           <a class="navbar-item" href="https://github.com/lxtGH/Video-K-Net">
            Video K-Net
          </a>
          <a class="navbar-item" href="https://www.mmlab-ntu.com/project/edgesam/">
            Edge-SAM
          </a>
          <a class="navbar-item" href="https://xushilin1.github.io/rap_sam/">
            RAP-SAM
          </a>
          <a class="navbar-item" href="https://github.com/lxtGH/Awesome-Segmentation-With-Transformer">
            Transformer Segmentation Survey
          </a>
          <a class="navbar-item" href="https://github.com/jianzongwu/Awesome-Open-Vocabulary">
            Open Vocabulary Learning Survey
          </a>
          <a class="navbar-item" href="https://lxtgh.github.io/project/omg_seg/">
            OMG-Seg
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding</h1>
          <div class="is-size-5 publication-authors">

            <span class="author-block">
              <a href="https://zhang-tao-whu.github.io/">Tao Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://lxtgh.github.io/">Xiangtai Li</a><sup>2,3</sup>,</span>
            <span class="author-block">
              <a href="https://haofei.vip/">Hao Fei</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://yuanhaobo.me/">Haobo Yuan</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://chocowu.github.io/">Shengqiong Wu</a><sup>2</sup>,</span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=FjoRmF4AAAAJ&hl=en">Shunping Ji</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://yanshuicheng.info/">Shuicheng Yan</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Wuhan University,</span>
            <span class="author-block"><sup>2</sup>Skywork AI,</span>
            <span class="author-block"><sup>3</sup>S-Lab, NTU</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2401.10229"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2401.10229"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/gX1MEFX791M"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lxtGH/OMG-Seg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./src/omg_llava_teaser.png" alt="empty">
        <p>
        The comprehensive capabilities of OMG-LLaVA. OMG-LLaVA can handle a variety of pixel-level, object-level, and image-level understanding and reasoning tasks with only one visual encoder, one visual decoder and one LLM.
        </p>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose OMG-LLaVA, a new and elegant framework combining powerful pixel-level vision understanding with reasoning abilities. It can accept various visual and text prompts for flexible user interaction.
        Specifically, we use a universal segmentation method as the visual encoder, integrating image information, perception priors, and visual prompts into visual tokens provided to the LLM. The LLM is responsible for understanding the user's text instructions and providing text responses and pixel-level segmentation results based on the visual information.
          </p>
          <p>
            OMG-LLaVA achieves image-level, object-level, and pixel-level reasoning and understanding in a single model, matching or surpassing the performance of specialized methods on multiple benchmarks.
            Rather than using LLM to connect each specialist, our work aims at end-to-end training on one encoder, one decoder, and one LLM.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <section class="setting teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model Scope Comparison</h2>
        <div class="content has-text-justified">
    <div class="hero-body">
        <img src="./src/comparison.jpg" alt="empty">
        <p>
          Comparison of capabilities of different models. We include several representative methods here. Our OMG-LLaVA offers the most comprehensive capabilities, encompassing image-level, object-level, and pixel-level understanding and reasoning.
          Compared to GlaMM and AnyRef, OMG-LLaVA features an elegant and simple system architecture with only a single visual encoder.
        </p>
    </div>
  </div>
      </div>
      </div>
  </div>

</section>


    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width="679" height="382" src="https://www.youtube.com/embed/gX1MEFX791M" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


    <section class="setting teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method: OMG-LLaVA</h2>
        <div class="content has-text-justified">
    <div class="hero-body">
        <img src="./src/method.jpg" alt="empty">
        <p>
          The Overview of OMG-LLaVA. OMG-LLaVA consists of OMG-Seg and LLM. OMG-Seg tokenizes the image into pixel-centric visual tokens, the detected objects, and inputs visual prompts into object-centric visual tokens.
          Additionally, the [SEG] token output by LLM is decoded by OMG-Seg into segmentation masks. OMG-Seg remains frozen at all stages.
          In particular, we present perception prior embedding method to enhance the pixel features with object prior before sending visual tokens to LLMs.
        </p>
    </div>
  </div>
      </div>
      </div>
  </div>

</section>


<section class="setting teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
    <div class="hero-body">
        <p>
          The comprehensive comparison of OMG-LLaVA and other MLLMs regarding pixel-level and object-level understanding and reasoning capability and performance.
          "-" indicates that the method does not handle this task. GLaMM used the GranD dataset for pretraining, which is significantly larger than
          the datasets used by other methods.
        </p>
        <img src="./src/overall_table.png" alt="empty">
    </div>

          <div class="hero-body">
        <p>
          Performance on referring expression segmentation datasets. The evaluation metric is cIoU. "ft"
          indicates finetuning on the referring expression datasets.
        </p>
        <img src="./src/res_table.png" alt="empty">
    </div>

         <div class="hero-body">
        <p>
          Performance on grounded conversation generation datasets. “ft” indicates finetuning on the GranDf
          dataset. † indicates that the method used the GranD dataset for pretraining.
        </p>
        <img src="./src/gcg_table.png" alt="empty">
    </div>

          <div class="hero-body">
        <p>
          Ablation study on RES and GCG datasets.
        </p>
        <img src="./src/ablation_table.png" alt="empty">
    </div>

  </div>
      </div>
      </div>
  </div>

</section>



<section class="setting teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demos</h2>
        <div class="content has-text-justified">
    <div class="hero-body">
        <img src="./src/res_compare.png" alt="empty">
      <p>
          Qualitative comparison on the referring expression segmentation task. LISA uses the 13B LLM,
          while GLaMM and our proposed OMG-LLaVA use the 7B LLM.
        </p>
    </div>

          <div class="hero-body">
        <img src="./src/gcg_compare1.png" alt="empty">
            <img src="./src/gcg_compare2.png" alt="empty">
      <p>
          Qualitative comparison on the grounded conversation generation task.
        </p>
    </div>

          <div class="hero-body">
        <img src="./src/chat_compare.png" alt="empty">
      <p>
          Qualitative comparison on the visual prompt-based description task.
        </p>
    </div>

          <div class="hero-body">
        <img src="./src/visual_prompt_compare.png" alt="empty">
      <p>
          Qualitative comparison on the image-based conversation task.
        </p>
    </div>

  </div>
      </div>
      </div>
  </div>

</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{OMGLLaVA,
  title={OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding},
  author={Zhang, Tao and Li, Xiangtai and Fei, Hao and Yuan, Haobo and Wu, Shengqiong and Ji, Shunping and Chen, Change Loy and Yan, Shuicheng},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks for the project template by  <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies Project</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
