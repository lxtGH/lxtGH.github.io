
<!DOCTYPE HTML>
<html lang="en">

<body>
  <style>
  .php{text-decoration:underline}
   </style>
  <table id="container">
    <tr>
      <td>
         * denotes equal contribution, mentored students by me are <a class="php">underline</a>.
        <table width="130%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>2023</heading>
                <ol>

                  <li>
                  <a href="">
                  <papertitle>Transformer-Based Visual Segmentation: A Survey.</papertitle>
                  </a>
                  <br>
                    <strong>Xiangtai Li</strong>, Henghui Ding, Wenwei Zhang, Haobo Yuan, Jiangmiao Pang, Guangliang Cheng, Kai Chen, Ziwei Liu, Chen Change Loy
                  <br>
                  In: <em>Arxiv (In Submission) </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2304.09854">Arxiv </a> |
                  <a href="https://github.com/lxtGH/Awesome-Segmenation-With-Transformer">Code</a>| <a href="https://www.mmlab-ntu.com/project/seg_survey/index.html">Project Page in MMlab@NTU</a>
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2301.00954">
                  <papertitle>PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation. </papertitle>
                  </a>
                  <br>
                        <strong>Xiangtai Li</strong>, Shilin Xu, Yibo Yang, Haobo Yuan, Guangliang Cheng, Yunhai Tong, Zhouchen Lin, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission), Extension of PanopticPartFormer</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.00954"></a> Paper |
                  <a href="https://github.com/lxtGH/Panoptic-PartFormer">Code</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2303.12782">
                  <papertitle>Tube-Link: A Flexible Cross Tube Baseline for Universal Video Segmentation.</papertitle>
                  </a>
                  <br>
                    <strong>Xiangtai Li</strong>, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, Chen Change Loy
                  <br>
                  In: <em>ICCV </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2303.12782">Arxiv </a> |
                  <a href="https://github.com/lxtGH/Tube-Link">Code</a>
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2207.04415">
                  <papertitle>SFNet: Faster and Accurate Semantic Segmentation via Semantic Flow.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Jiangning Zhang, Yibo Yang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>IJCV</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2207.04415">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2301.00805">
                  <papertitle>Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation.</papertitle>
                  </a>
                  <br>
                      <a class="php">Jianzong Wu*</a>, <strong>Xiangtai Li*</strong>, Henghui Ding, Xia Li, Guangliang Cheng, Yunhai Tong, Chen Change Loy
                  <br>
                  In: <em>ICCV</em>, 2023.
                  <br>
                    <a href="https://arxiv.org/abs/2301.00805">Arxiv </a> |
                  <a href="https://github.com/jzwu48033552/betrayed-by-captions">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2301.01146">
                  <papertitle>Rethinking Mobile Block for Efficient Neural Models.</papertitle>
                  </a>
                  <br>
                    Jiangning Zhang, <strong>Xiangtai Li</strong>, Jian Li, Liang Liu, Zhucun Xue, Boshen Zhang, Zhengkai Jiang, Tianxin Huang, Yabiao Wang, Chengjie Wang
                  <br>
                  In: <em>ICCV</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.01146">Arxiv</a> |
                  <a href="https://github.com/zhangzjn/EMO">Code</a> |
                  </li>
                  <p> </p>


                    <li>
                  <a href="">
                  <papertitle>Explore In-Context Learning for 3D Point Cloud Understanding.</papertitle>
                  </a>
                  <br>
                   Zhongbin Fang, <strong>Xiangtai Li</strong>,  Xia Li, Joachim M. Buhmann, Chen Change Loy, Mengyuan Liu
                  <br>
                  In: <em>NeurIPS &nbsp <font color="red"><strong>(Spotlight)</strong></font></em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2306.08659">Arxiv </a> |
                  <a href="https://github.com/fanglaosi/Point-In-Context">Code</a></li>
                  <p> </p>


                    <li>
                  <a href="">
                  <papertitle>4D Panoptic Scene Graph Generation.</papertitle>
                  </a>
                  <br>
                   Jingkang Yang, Jun Chen, Wenxuan Peng, Shuai Liu, Fangzhou Hong, <strong>Xiangtai Li</strong>, Kaiyang Zhou, Qifeng Chen, Ziwei Liu
                  <br>
                  In: <em>NeurIPS &nbsp <font color="red"><strong>(Spotlight)</strong></font></em>, 2023.
                  <br>
                  <a href="">Arxiv </a> |
                  <a href="">Code</a></li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2301.01146">
                  <papertitle> Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision.</papertitle>
                  </a>
                  <br>
                   Menghao Li, Chunlei Wang, Wenquan Feng, Shuchang Lyu, Guangliang Cheng, <strong>Xiangtai Li</strong>, Binghao Liu, Qi Zhao
                  <br>
                  In: <em>ICCV workshop</em>, 2023 <font color="red"><strong>(Oral Presentation)</strong></font>.
                  <br>
                  <a href="https://arxiv.org/abs/2307.12392">Arxiv</a> |
                  <a href="https://github.com/cv516Buaa/IR-VG">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2301.01146">
                  <papertitle>Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants
.</papertitle>
                  </a>
                  <br>
                    Yibo Yang, Haobo Yuan, <strong>Xiangtai Li</strong>, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip Torr, Dacheng Tao, Bernard Ghanem
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2308.01746">Arxiv</a> |
                  <a href="">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2205.14354">
                  <papertitle>Multi-Task Learning with Multi-Query Transformer for Dense Prediction.</papertitle>
                  </a>
                  <br>
                    <a class="php">Yangyang Xu*</a>, <strong>Xiangtai Li*</strong>, Haobo Yuan, Yibo Yang, Jing Zhang, Yunhai Tong, Lefei Zhang, Dacheng Tao
                  <br>
                  In: <em>IEEE-TCSVT </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2205.14354">Arxiv</a> |
                    <a href="https://github.com/yangyangxu0/MQTransformer">Code</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="">
                  <papertitle>Panoptic Video Scene Graph Generation.</papertitle>
                  </a>
                  <br>
                   Jingkang Yang, Wenxuan Peng, <strong>Xiangtai Li</strong>, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Wayne Zhang, Kaiyang Zhou, Chen Change Loy, Ziwei Liu.
                  <br>
                  In: <em>CVPR </em>, 2023.
                  <br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf">Paper </a> |
                  <a href="https://github.com/Jingkang50/OpenPVSG">Code</a></li>
                  <p> </p>



                  <li>
                  <a href="">
                  <papertitle>Towards Open Vocabulary Learning: A Survey.</papertitle>
                  </a>
                  <br>
                   Jianzong Wu, <strong>Xiangtai Li</strong>, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission) </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2306.15880">Arxiv </a> |
                  <a href="https://github.com/jianzongwu/Awesome-Open-Vocabulary">Project</a></li>
                  <p> </p>

                  <li>
                  <a href="">
                  <papertitle>MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation
</papertitle>
                  </a>
                  <br>
                    Jiahao Xie, Wei Li, <strong> Xiangtai Li </strong>, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                  <br>
                  In: <em>Arxiv (In Submission) </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2309.13042">Arxiv </a> |
                  <a href="https://github.com/Jiahao000/MosaicFusion">Project</a></li>
                  <p> </p>



                  <li>
                  <a href="https://openreview.net/forum?id=y5W8tpojhtJ">
                  <papertitle>Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning.</papertitle>
                  </a>
                  <br>
                 Yibo Yang, Haobo Yuan,  <strong>Xiangtai Li </strong>, Zhouchen Lin, Philip Torr, Dacheng Tao
                  <br>
                  In: <em>ICLR (spotlight)</em>, 2023.
                  <br>
                  <a href="">Arxiv </a> |
                  <a href="">Code</a></li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2301.01156">
                  <papertitle>Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation.</papertitle>
                  </a>
                  <br>
                  <a class="php"> Yue Han</a>, Jiangning Zhang, Zhucun Xue, Chao Xu, Xintian Shen, Yabiao Wang, Chengjie Wang, Yong Liu, <strong>Xiangtai Li</strong>
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.01156">Arxiv </a> |
                  <a href="https://github.com/hanyue1648/RefT">Code</a></li>
                  <p> </p>



                </ol>

                <heading>2022</heading>
                <ol>

                  <li>
                  <a href="https://arxiv.org/abs/2201.05047">
                  <papertitle>TransVOD: End-to-End Video Object Detection with Spatial-Temporal Transformers.</papertitle>
                  </a>
                  <br>
                   <a class="php">Qianyu Zhou*</a>, <strong>Xiangtai Li*</strong>, Lu He, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lizhuang Ma, Dacheng Tao
                  <br>
                  In: <em>IEEE-T-PAMI</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2201.05047">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2107.13155">
                  <papertitle>Improving Video Instance Segmentation via Temporal Pyramid Routing.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Hao He, Yibo Yang, Henghui Ding, Kuiyuan Yang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>IEEE-T-PAMI</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2107.13155">Arxiv</a> |
                  <a href="https://github.com/lxtGH/TemporalPyramidRouting">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2204.04656">
                  <papertitle>Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li*</strong>, Wenwei Zhang*, Jiangmiao Pang*, Kai Chen, Guangliang Cheng, Yunhai Tong, Chen Change Loy
                  <br>
                  In: <em>CVPR</em>, 2022.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2204.04656">Arxiv</a> |
                  <a href="https://github.com/lxtGH/Video-K-Net">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2204.04655">
                  <papertitle>Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Shilin Xu, Yibo Yang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2204.04655">Arxiv</a> |
                  <a href="https://github.com/lxtGH/Panoptic-PartFormer">Code</a> |
                  </li>

                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2204.04654">
                  <papertitle>Fashionformer: A Simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition.</papertitle>
                  </a>
                  <br>
                    <a class="php">Shilin Xu*</a>, <strong>Xiangtai Li*</strong>, Jingbo Wang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2204.04654">Arxiv</a> |
                  <a href="https://github.com/xushilin1/FashionFormer">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2112.02582">
                  <papertitle>PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation.</papertitle>
                  </a>
                  <br>
                   <a class="php">Haobo Yuan*</a>, <strong>Xiangtai Li*</strong>, Yibo Yang, Guangliang Cheng, Jing Zhang, Yunhai Tong, Lefei Zhang, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2112.02582">Arxiv</a> |
                  <a href="https://github.com/HarborYuan/PolyphonicFormer">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2203.09081">
                  <papertitle>Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?</papertitle>
                  </a>
                  <br>
                     Yibo Yang, Shixiang Chen, <strong>Xiangtai Li</strong>, Liang Xie, Zhouchen Lin, Dacheng Tao
                  <br>
                  In: <em>NeurIPS</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2203.09081">Arxiv</a> |
                  <a href="https://github.com/NeuralCollapseApplications/ImbalancedLearning">Code</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2206.09325">
                  <papertitle>EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm.</papertitle>
                  </a>
                  <br>
                  Jiangning Zhang*, <strong>Xiangtai Li*</strong>, Yabiao Wang, Chengjie Wang, Yibo Yang, Yong Liu, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2206.09325">Arxiv</a> |
                  <a href="https://https://github.com/zhangzjn/EATFormer">Code</a> |
                  </li>
                  <p> </p>



                  <li>
                  <a href="https://arxiv.org/abs/2209.09554">
                  <papertitle>Towards Robust Referring Image Segmentation.</papertitle>
                  </a>
                  <br>
                  <a class="php">Jianzong Wu*</a>, <strong>Xiangtai Li*</strong>, Xia Li, Henghui Ding, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2209.09554">Arxiv</a> |
                  <a href="https://lxtgh.github.io/project/robust_ref_seg/">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2212.08330">
                  <papertitle>Convolution-enhanced Evolving Attention Networks.</papertitle>
                  </a>
                  <br>
                    Yujing Wang, Yaming Yang, Zhuo Li, Jiangang Bai, Mingliang Zhang, <strong>Xiangtai Li</strong>, Jing Yu, Ce Zhang, Gao Huang, Yunhai Tong
                  <br>
                  In: <em>IEEE-T-PAMI</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2212.08330"></a> Paper |
                  <a href="https://github.com/pkuyym/EvolvingAttention">Code</a> |
                  </li>
                  <p> </p>




                </ol>

                <heading>2021</heading>
                <ol>
                  <li>
                  <a href="https://arxiv.org/abs/2105.10920">
                  <papertitle>End-to-End Video Object Detection with Spatial-Temporal Transformers.</papertitle>
                  </a>
                  <br>  <a class="php">Lu He*</a>, Qianyu Zhou*, <strong>Xiangtai Li*</strong>, Li Niu, Guangliang Cheng, Xiao Li, Wenxuan Liu, Yunhai Tong, Lizhuang Ma, Liqing Zhang  <br>
                  In: <em>ACM-MM</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.10920">Arxiv</a> |
                  <a href="https://github.com/SJTU-LuHe/TransVOD">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2105.11651">
                  <papertitle>Fast and Accurate Scene Parsing via Bi-direction Alignment Networks.</papertitle>
                  </a>
                  <br>  <a class="php">Chen Shi</a>, <strong>Xiangtai Li</strong>, Yanran Wu, Yunhai Tong, Yi Xu  <br>
                  In: <em>ICIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11651">Arxiv</a> |
                  <a href="https://github.com/jojacola/BiAlignNet">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2105.11657">
                  <papertitle>Dynamic Dual Sampling Module for Fine-Grained Semantic Segmentation.</papertitle>
                  </a>
                    <br>  <a class="php">Yanran Wu</a>,  <strong>Xiangtai Li</strong>, Chen Shi, Yunhai Tong, Yang Hua, Tao Song, Ruhui Ma, Haibing Guan  <br>
                  In: <em>ICIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11657">Arxiv</a> |
                  <a href="https://github.com/Fantasticarl/DDSM">Code</a> </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2105.11668">
                  <papertitle>BoundarySqueeze: Image Segmentation as Boundary Squeezing.</papertitle>
                  </a>
                  <br>  <a class="php">Hao He*</a>, <strong>Xiangtai Li*</strong>, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lubin Weng, Shiming Xiang, Dacheng Tao  <br>
                  In: <em>Arxiv (in submission)</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11668">Arxiv</a> |
                  <a href="https://github.com/lxtGH/BSSeg">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2103.15734">
                  <papertitle>Enhanced Boundary Learning for Glass-like Object Segmentation.</papertitle>
                  </a>
                    <br>  <a class="php">Hao He*</a>, <strong>Xiangtai Li*</strong>, Guangliang Cheng, Jianping Shi, Yunhai Tong, Gaofeng Meng, Veronique Prinet, Lubin Weng  <br>
                  In: <em>ICCV</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.15734">Arxiv</a> |
                  <a href="https://github.com/hehao13/EBLNet">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2103.06564">
                  <papertitle>PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation.</papertitle>
                  </a>
                  <br>   <strong>Xiangtai Li</strong>*, Hao He*, Xia Li, Duo Li, Guangliang Cheng, Jianping Shi, Lubin Weng, Yunhai Tong, Zhouchen Lin  <br>
                  In: <em>CVPR</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.06564">Arxiv</a> |
                  <a href="https://github.com/lxtGH/PFSegNets">Code</a> |
                  <a href="https://github.com/Jittor/PFSegNets-Jittor">Jitter Code</a>
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2103.06255">
                  <papertitle>Involution: Inverting the Inherence of Convolution for Visual Recognition.</papertitle>
                  </a>
                  <br>  Duo Li, Jie Hu, Changhu Wang, <strong>Xiangtai Li</strong>, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen  <br>
                  In: <em>CVPR</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.06255">Arxiv</a> |
                  <a href="https://github.com/d-li14/involution">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2011.03308">
                  <papertitle>Towards Efficient Scene Understanding via Squeeze Reasoning.</papertitle>
                  </a>
                  <br>  <strong>Xiangtai Li</strong>, Xia Li, Ansheng You, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Zhouchen Lin <br>
                  In: <em>IEEE-TIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2011.03308">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2107.13154">
                  <papertitle>Global Aggregation then Local Distribution for Scene Parsing.</papertitle>
                  </a>
                  <br>  <strong>Xiangtai Li</strong>, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Xiatian Zhu, Tao Xiang <br>
                  In: <em>IEEE-TIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2107.13154">Arxiv</a> |
                  <a href="https://github.com/lxtGH/GALD-DGCNet">Code</a> </li>
                  <p> </p>

                </ol>


                <heading>2020</heading>
                <ol>
                    <li>
                  <a href="https://arxiv.org/abs/2002.10120">
                  <papertitle>Semantic Flow for Fast and Accurate Scene Parsing.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Ansheng You, Zhen Zhu, Houlong Zhao, Maoke Yang, Kuiyuan Yang, Yunhai Tong
                  <br>
                  In: <em>ECCV</em>, 2020.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2002.10120">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> |
                        <a href="https://github.com/donnyyou/torchcv">Code (torchcv)</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2007.10035">
                  <papertitle>Improving Semantic Segmentation via Decoupled Body and Edge Supervision.</papertitle>
                  </a>
                  <br>
                   <strong>Xiangtai Li </strong>, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, Yunhai Tong
                  <br>
                  In: <em>ECCV</em>, 2020.
                  <br>
                  <a href="https://arxiv.org/abs/2007.10035">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/1904.01803">
                  <papertitle>GFF: Gated Fully Fusion for Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Houlong Zhao, Lei Han, Yunhai Tong, Kuiyuan Yang
                  <br>
                  In: <em>AAAI</em>, 2020.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/1904.01803">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a> |
                  </li>
                  <p> </p>

                </ol>


                <heading>2019</heading>
                <ol>
                  <li>
                  <a href="https://arxiv.org/abs/1909.07229">
                  <papertitle>Global Aggregation then Local Distribution in Fully Convolutional Networks.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai Tong
                  <br>
                  In: <em>BMVC</em>, 2019.
                  <a href="https://arxiv.org/abs/1909.07229">Paper</a> |
                  <a href="hhttps://github.com/lxtGH/GALD-DGCNet">Code</a> |
                  <p> </p>

                    <li>
                  <a href="https://arxiv.org/abs/1909.06121">
                  <papertitle>Dual Graph Convolutional Network for Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  Li Zhang*, <strong>Xiangtai Li*</strong>, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, Philip H.S. Torr
                  <br>
                  In: <em>BMVC</em>, 2019.
                  <a href="https://arxiv.org/abs/1909.06121">Paper</a> |
                  <a href="hhttps://github.com/lxtGH/GALD-DGCNet">Code</a> |
                  <p> </p>

                    <li>
                  <a href="../paper_local/Flow2Seg_motion.pdf">
                  <papertitle>Flo2seg: Motion-Aided Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Jiangang Bai, Yunhai Tong, Kuiyuan Yang
                  <br>
                  In: <em>IJCNN</em>, 2019. long paper
                  <a href="../paper_local/Flow2Seg_motion.pdf">Paper</a> |
                  <p> </p>

                </ol>
              </td>
            </tr>
            </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                      <td>
                        <br>
                        <p align="center">
                          <font size="2">

                            Based on a template by <a href="https://jonbarron.info">Jon Barron</a>

                            </font>
                        </p>
                      </td>
                    </tr>
                  </table>
                  </td>
              </tr>
            </table>
          </body>
          </html>