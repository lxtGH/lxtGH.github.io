
<!DOCTYPE HTML>
<html lang="en">

<body>
  <style>
  .php{text-decoration:underline}
   </style>
  <table id="container">
    <tr>
      <td>
         * denotes equal contribution, mentored students by me are <a class="php">underline</a>.
        <table width="130%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>

                  <heading>2024</heading>
                  <ol>

                      <li>
                  <a href="">
                  <papertitle>OMG-Seg: Is One Model Good Enough For All Segmentation?</papertitle>
                  </a>
                  <br>
                          <strong>Xiangtai Li</strong>, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy
                  <br>
                  In: <em>CVPR </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2401.10229">Arxiv </a> |
                  <a href="https://github.com/lxtGH/OMG-Seg">Code</a>
                          <a href="https://lxtgh.github.io/project/omg_seg/">Project Page</a>
                  </li>
                  <p> </p>


                      <li>
                  <a href="">
                  <papertitle>Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively.</papertitle>
                  </a>
                  <br>
                          <a class="php">Haobo Yuan</a>, <strong>Xiangtai Li</strong>, Chong Zhou, Yining Li, Kai Chen, Chen Change Loy
                  <br>
                  In: <em>Arxiv </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2401.02955">Arxiv </a> |
                  <a href="https://github.com/HarborYuan/ovsam">Code</a>
                          <a href="https://www.mmlab-ntu.com/project/ovsam">Project Page</a>
                  </li>
                  <p> </p>

                      <li>
                  <a href="">
                  <papertitle>Point Cloud Mamba: Point Cloud Learning via State Space Model.</papertitle>
                  </a>
                  <br>
                            <a class="php">Tao Zhang</a>, <strong>Xiangtai Li</strong>, Haobo Yuan, Shunping Ji, Shuicheng Yan
                  <br>
                  In: <em>Arxiv </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2403.00762">Arxiv </a> |
                  <a href="https://github.com/zhang-tao-whu/PCM">Code</a>
                  </li>
                  <p> </p>

                  <li>
                  <a href="">
                  <papertitle>DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries.</papertitle>
                  </a>
                  <br>
                         Yikang Zhou,  <a class="php">Tao Zhang</a>, Shunping Ji, Shuicheng Yan, <strong>Xiangtai Li</strong>
                  <br>
                  In: <em>Arxiv </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2404.00086">Arxiv </a> |
                  <a href="https://github.com/SkyworkAI/DAQ-VS">Code</a>
                  </li>
                  <p> </p>


                      <li>
                  <a href="">
                  <papertitle>Towards Language-Driven Video Inpainting via Multimodal Large Language Models.</papertitle>
                  </a>
                  <br>
                          <a class="php">Jianzong Wu</a>, <strong>Xiangtai Li</strong>, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy
                  <br>
                  In: <em>CVPR </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2401.10226">Arxiv </a> |
                  <a href="https://github.com/jianzongwu/Language-Driven-Video-Inpainting">Code</a>
                          <a href="https://jianzongwu.github.io/projects/rovi/">Project Page</a>
                  </li>
                  <p> </p>


                      <li>
                  <a href="">
                  <papertitle>RAP-SAM: Towards Real-Time All-Purpose Segment Anything.</papertitle>
                  </a>
                  <br>
                          <a class="php">Shilin Xu<a>, Haobo Yuan, Qingyu Shi, Lu Qi, Jingbo Wang, Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard Ghanem, <strong>Xiangtai Li</strong>, Ming-Hsuan Yang
                  <br>
                  In: <em>Arxiv </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2401.10228">Arxiv </a> |
                  <a href="https://github.com/xushilin1/RAP-SAM/">Code</a>
                              <a href="https://xushilin1.github.io/rap_sam/">Project Page</a>

                  <p> </p>

                              <li>
                  <a href="">
                  <papertitle>Generalizable Entity Grounding via Assistance of Large Language Model</papertitle>
                  </a><br> Lu Qi, Yi-Wen Chen, Lehan Yang, Tiancheng Shen, <strong> Xiangtai Li </strong>, Weidong Guo, Yu Xu, Ming-Hsuan Yang<br>
                  In: <em>Arxiv </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2402.02555">Arxiv </a>
                  </li>
                  <p> </p>


                      <li>
                  <a href="">
                  <papertitle>BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model.</papertitle>
                  </a>
                  <br>
                          <a class="php">Yiran Song</a>, <a class="php">Qianyu Zhou</a>, <strong>Xiangtai Li</strong>, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma
                  <br>
                  In: <em>CVPR </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2401.02317">Arxiv </a> |
                  <a href="https://github.com/zongzi13545329/BA-SAM">Code</a>
                  </li>
                  <p> </p>

                              <li>
                  <a href="">
                  <papertitle>RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation.</papertitle>
                  </a>
                  <br>
                        Peng Lu, Tao Jiang, Yining Li, <strong>Xiangtai Li</strong>, Kai Chen, Wenming Yang
                  <br>
                  In: <em>CVPR </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2312.07526">Arxiv </a> |
                  <a href="https://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmo">Code</a></li>
                  <p> </p>

                      <li>
                  <a href="">
                  <papertitle>A Generalist FaceX via Learning Unified Facial Representation.</papertitle>
                  </a>
                  <br>
                    Yue Han, Jiangning Zhang, Junwei Zhu, <strong>Xiangtai Li</strong>, Yanhao Ge, Wei Li, Chengjie Wang, Yong Liu, Xiaoming Liu, Ying Tai
                  <br>
                  In: <em>Arxiv (In Submission) </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2401.00551">Arxiv </a> |
                  <a href="https://github.com/diffusion-facex/FaceX">Code</a>
                  </li>
                  <p> </p>

                              <li>
                  <a href="">
                  <papertitle>Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning.</papertitle>
                  </a>
                  <br>
                        Xinshun Wang, Zhongbin Fang, Xia Li, <strong>Xiangtai Li</strong>, Chen Chen, Mengyuan Liu
                  <br>
                  In: <em>CVPR </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2312.03703">Arxiv </a> |
                  <a href="https://github.com/fanglaosi/Skeleton-in-Context">Code</a></li>
                  <p> </p>



                      <li>
                  <a href="https://arxiv.org/abs/2301.01146">
                  <papertitle>CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction.</papertitle>
                  </a>
                  <br>
                        Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, <strong>Xiangtai Li</strong>, Wentao Liu, Chen Change Loy
                  <br>
                  In: <em>ICLR &nbsp <font color="red"><strong>(Spotlight)</strong></font></em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2310.01403">Arxiv</a> |
                  <a href="https://github.com/wusize/CLIPSelf">Code</a> |
                  </li>
                  <p> </p>

                              <li>
                  <a href="">
                  <papertitle>Towards Open-Vocabulary Learning: A Survey.</papertitle>
                  </a>
                  <br>
                   <a class="php">Jianzong Wu*</a>, <strong>Xiangtai Li*</strong>, Shilin Xu*, Haobo Yuan*, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem, Dacheng Tao
                  <br>
                  In: <em>IEEE T-PAMI </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2306.15880">Arxiv </a> |
                  <a href="https://github.com/jianzongwu/Awesome-Open-Vocabulary">Project</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2206.09325">
                  <papertitle>EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm.</papertitle>
                  </a>
                  <br>
                  Jiangning Zhang*, <strong>Xiangtai Li*</strong>, Yabiao Wang, Chengjie Wang, Yibo Yang, Yong Liu, Dacheng Tao
                  <br>
                  In: <em>IJCV</em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2206.09325">Arxiv</a> |
                  <a href="https://https://github.com/zhangzjn/EATFormer">Code</a> |
                  </li>
                  <p> </p>

                      <li>
                  <a href="">
                  <papertitle>An Open and Comprehensive Pipeline for Unified Object Grounding and Detection.</papertitle>
                  </a>
                  <br>
                    Xiangyu Zhao, Yicheng Chen, Shilin Xu, <strong>Xiangtai Li</strong>, Xinjiang Wang, Yining Li, Haian Huang
                  <br>
                          <em>Technical Report For MM-Grounding DINO </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2401.02361">Arxiv </a> |
                  <a href="https://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino">Code</a>
                  </li>
                  <p> </p>

                  <li>
                  <a href="">
                  <papertitle>GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning.</papertitle>
                  </a>
                  <br>
                        Xiaojie Li, Yibo Yang, <strong>Xiangtai Li</strong>, Jianlong Wu, Yue Yu, Bernard Ghanem, Min Zhang
                  <br>
                          <em>Technical Report </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2403.12003">Arxiv </a> |
                  <a href="https://github.com/xiaojieli0903/genview">Code</a>
                  </li>
                  <p> </p>


                 <li>
                  <a href="">
                  <papertitle>Explore In-Context Segmentation via Latent Diffusion Models.</papertitle>
                  </a>
                  <br>
                  <a class="php"> Chaoyang Wang </a>, <strong>Xiangtai Li</strong>, Henghui Ding, Lu Qi, Jiangning Zhang, Yunhai Tong, Chen Change Loy, Shuicheng Yan
                  <br>
                          <em>Technical Report </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2403.09616">Arxiv </a> |
                  <a href="https://wang-chaoyang.github.io/project/refldmseg">Project</a>
                  </li>
                  <p> </p>

                              <li>
                  <a href="">
                  <papertitle>MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection.</papertitle>
                  </a>
                  <br>
                  Haoyang He, Yuhu Bai, Jiangning Zhang, Qingdong He, Hongxu Chen, Zhenye Gan, Chengjie Wang, <strong>Xiangtai Li</strong>, Guanzhong Tian, Lei Xie
                  <br>
                          <em>Technical Report </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2404.06564">Arxiv </a> |
                  <a href="https://lewandofskee.github.io/projects/MambaAD/">Project</a>
                  </li>
                  <p> </p>


                  <li>
                  <a href="">
                  <papertitle>Learning Feature Inversion for Multi-class Anomaly Detection under General-purpose COCO-AD Benchmark.</papertitle>
                  </a>
                  <br>
                      Jiangning Zhang, Chengjie Wang, <strong>Xiangtai Li</strong>, Guanzhong Tian, Zhucun Xue, Yong Liu, Guansong Pang, Dacheng Tao
                      <br>
                          <em>Technical Report </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2404.10760">Arxiv </a> |
                  <a href="https://github.com/zhangzjn/ader">Project</a>
                  </li>
                  <p> </p>

                              <li>
                  <a href="">
                  <papertitle>DGMamba: Domain Generalization via Generalized State Space Model.</papertitle>
                  </a>
                  <br>
                                  Shaocong Long, Qianyu Zhou, <strong>Xiangtai Li</strong>, Xuequan Lu, Chenhao Ying, Yuan Luo, Lizhuang Ma, Shuicheng Yan
                  <br>
                          <em>Technical Report </em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2404.07794">Arxiv </a> |
                  <a href="https://github.com/longshaocong/DGMamba">Project</a>
                  </li>
                  <p> </p>


                  <li>
                  <a href="">
                  <papertitle>VG4D: Vision-Language Model Goes 4D Video Recognition.</papertitle>
                  </a>
                      <br> <a class="php">Zhichao Deng</a>, <strong>Xiangtai Li</strong>, Xia Li, Yunhai Tong, Shen Zhao, Mengyuan Liu
                  <br>
                  In: <em>ICRA</em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2404.11605">Arxiv</a> |
                  <a href="https://github.com/Shark0-0/VG4D">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="">
                  <papertitle>Point-In-Context: Understanding Point Cloud via In-Context Learning.</papertitle>
                  </a>
                      <br> Mengyuan Liu, <a class="php">Zhongbin Fang</a>, Xia Li, Joachim M. Buhmann, <strong>Xiangtai Li</strong>, Chen Change Loy
                  <br>
                  In: <em>Arxiv (Extension of Point-In-Context)</em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2404.12352">Arxiv</a> |
                  <a href="https://fanglaosi.github.io/Point-In-Context_Pages/">Code</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2209.09554">
                  <papertitle>Towards Robust Referring Image Segmentation.</papertitle>
                  </a>
                  <br>
                  <a class="php">Jianzong Wu*</a>, <strong>Xiangtai Li*</strong>, Xia Li, Henghui Ding, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>TIP</em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2209.09554">Arxiv</a> |
                  <a href="https://lxtgh.github.io/project/robust_ref_seg/">Code</a> |
                  </li>
                  <p> </p>


                  </ol>

                <heading>2023</heading>
                <ol>

                  <li>
                  <a href="">
                  <papertitle>Transformer-Based Visual Segmentation: A Survey.</papertitle>
                  </a>
                  <br>
                    <strong>Xiangtai Li</strong>, Henghui Ding, Wenwei Zhang, Haobo Yuan, Jiangmiao Pang, Guangliang Cheng, Kai Chen, Ziwei Liu, Chen Change Loy
                  <br>
                  In: <em>IEEE T-PAMI (minor) </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2304.09854">Arxiv </a> |
                  <a href="https://github.com/lxtGH/Awesome-Segmenation-With-Transformer">Code</a>| <a href="https://www.mmlab-ntu.com/project/seg_survey/index.html">Project Page in MMlab@NTU</a>
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2301.00954">
                  <papertitle>PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation. </papertitle>
                  </a>
                  <br>
                        <strong>Xiangtai Li</strong>, Shilin Xu, Yibo Yang, Haobo Yuan, Guangliang Cheng, Yunhai Tong, Zhouchen Lin, Ming-Hsuan Yang, Dacheng Tao
                  <br>
                  In: <em>IEEE T-PAMI (major), Extension of PanopticPartFormer</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.00954"></a> Paper |
                  <a href="https://github.com/lxtGH/Panoptic-PartFormer">Code</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2303.12782">
                  <papertitle>Tube-Link: A Flexible Cross Tube Baseline for Universal Video Segmentation.</papertitle>
                  </a>
                  <br>
                    <strong>Xiangtai Li</strong>, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, Chen Change Loy
                  <br>
                  In: <em>ICCV </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2303.12782">Arxiv </a> |
                  <a href="https://github.com/lxtGH/Tube-Link">Code</a>
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2207.04415">
                  <papertitle>SFNet: Faster and Accurate Semantic Segmentation via Semantic Flow.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Jiangning Zhang, Yibo Yang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>IJCV</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2207.04415">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2301.00805">
                  <papertitle>Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation.</papertitle>
                  </a>
                  <br>
                      <a class="php">Jianzong Wu*</a>, <strong>Xiangtai Li*</strong>, Henghui Ding, Xia Li, Guangliang Cheng, Yunhai Tong, Chen Change Loy
                  <br>
                  In: <em>ICCV</em>, 2023.
                  <br>
                    <a href="https://arxiv.org/abs/2301.00805">Arxiv </a> |
                  <a href="https://github.com/jzwu48033552/betrayed-by-captions">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2301.01146">
                  <papertitle>Rethinking Mobile Block for Efficient Neural Models.</papertitle>
                  </a>
                  <br>
                    Jiangning Zhang, <strong>Xiangtai Li</strong>, Jian Li, Liang Liu, Zhucun Xue, Boshen Zhang, Zhengkai Jiang, Tianxin Huang, Yabiao Wang, Chengjie Wang
                  <br>
                  In: <em>ICCV</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.01146">Arxiv</a> |
                  <a href="https://github.com/zhangzjn/EMO">Code</a> |
                  </li>
                  <p> </p>


                    <li>
                  <a href="https://arxiv.org/abs/2301.01146">
                  <papertitle>EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM.</papertitle>
                  </a>
                  <br>
                        <a class="php">Chong Zhou</a>, <strong>Xiangtai Li</strong>, Chen Change Loy, Bo Dai
                  <br>
                  In: <em>Arxiv</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2312.06660">Arxiv</a> |
                  <a href="https://github.com/chongzhou96/EdgeSAM">Code</a> |
                  </li>
                  <p> </p>



                    <li>
                  <a href="">
                  <papertitle>Explore In-Context Learning for 3D Point Cloud Understanding.</papertitle>
                  </a>
                  <br>
                        <a class="php">Zhongbin Fang</a>, <strong>Xiangtai Li</strong>,  Xia Li, Joachim M. Buhmann, Chen Change Loy, Mengyuan Liu
                  <br>
                  In: <em>NeurIPS &nbsp <font color="red"><strong>(Spotlight)</strong></font></em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2306.08659">Arxiv </a> |
                  <a href="https://github.com/fanglaosi/Point-In-Context">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://openreview.net/forum?id=y5W8tpojhtJ">
                  <papertitle>Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning.</papertitle>
                  </a>
                  <br>
                    Yibo Yang, Haobo Yuan,  <strong>Xiangtai Li </strong>, Zhouchen Lin, Philip Torr, Dacheng Tao
                  <br>
                  In: <em>ICLR &nbsp <font color="red"><strong>(Spotlight)</strong></font> </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2302.03004">Arxiv </a> |
                  <a href="https://github.com/NeuralCollapseApplications/FSCIL">Code</a></li>
                  <p> </p>

                    <li>
                  <a href="">
                  <papertitle>4D Panoptic Scene Graph Generation.</papertitle>
                  </a>
                  <br>
                   Jingkang Yang, Jun Chen, Wenxuan Peng, Shuai Liu, Fangzhou Hong, <strong>Xiangtai Li</strong>, Kaiyang Zhou, Qifeng Chen, Ziwei Liu
                  <br>
                  In: <em>NeurIPS &nbsp <font color="red"><strong>(Spotlight)</strong></font></em>, 2023.
                  <br>
                  <a href="">Arxiv </a> |
                  <a href="">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2307.12392">
                  <papertitle> Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision.</papertitle>
                  </a>
                  <br>
                   Menghao Li, Chunlei Wang, Wenquan Feng, Shuchang Lyu, Guangliang Cheng, <strong>Xiangtai Li</strong>, Binghao Liu, Qi Zhao
                  <br>
                  In: <em>ICCV workshop</em>, 2023 <font color="red"><strong>(Oral Presentation, Best Paper Nomination)</strong></font>.
                  <br>
                  <a href="https://arxiv.org/abs/2307.12392">Arxiv</a> |
                  <a href="https://github.com/cv516Buaa/IR-VG">Code</a> |
                  </li>
                  <p> </p>



                    <li>
                  <a href="https://arxiv.org/abs/2310.01393">
                  <papertitle> DST-Det: Simple Dynamic Self-Training for Open-Vocabulary Object Detection.</papertitle>
                  </a>
                  <br>
                        <a class="php">Shilin Xu</a>*, <strong>Xiangtai Li*</strong>, Size Wu, Wenwei Zhang, Yining Li, Guangliang Cheng, Yunhai Tong, Kai Chen, Chen Change Loy
                  <br>
                  In: <em>Arxiv</em>, .
                  <br>
                  <a href="https://arxiv.org/abs/2310.01393">Arxiv</a> |
                  <a href="https://github.com/xushilin1/dst-det">Code</a> |
                  </li>
                  <p> </p>



                  <li>
                  <a href="https://arxiv.org/abs/2301.01146">
                  <papertitle>Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants
.</papertitle>
                  </a>
                  <br>
                    Yibo Yang, Haobo Yuan, <strong>Xiangtai Li</strong>, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip Torr, Dacheng Tao, Bernard Ghanem
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2308.01746">Arxiv</a> |
                  <a href="https://github.com/NeuralCollapseApplications/UniCIL">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2205.14354">
                  <papertitle>Multi-Task Learning with Multi-Query Transformer for Dense Prediction.</papertitle>
                  </a>
                  <br>
                    <a class="php">Yangyang Xu*</a>, <strong>Xiangtai Li*</strong>, Haobo Yuan, Yibo Yang, Jing Zhang, Yunhai Tong, Lefei Zhang, Dacheng Tao
                  <br>
                  In: <em>IEEE-TCSVT </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2205.14354">Arxiv</a> |
                    <a href="https://github.com/yangyangxu0/MQTransformer">Code</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="">
                  <papertitle>Panoptic Video Scene Graph Generation.</papertitle>
                  </a>
                  <br>
                   Jingkang Yang, Wenxuan Peng, <strong>Xiangtai Li</strong>, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Wayne Zhang, Kaiyang Zhou, Chen Change Loy, Ziwei Liu.
                  <br>
                  In: <em>CVPR </em>, 2023.
                  <br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf">Paper </a> |
                  <a href="https://github.com/LilyDaytoy/OpenPVSG">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="">
                  <papertitle>Pair then Relation: Pair-Net for Panoptic Scene Graph Generation.</papertitle>
                  </a>
                  <br>
                      <a class="php">Jinghao Wang</a>, Zhengyu Wen, <strong>Xiangtai Li</strong>, Zujin Guo, Jingkang Yang, Ziwei Liu
                  <br>
                  In: <em>IEEE T-PAMI (minor) </em>, 2023.
                  <br>
                    <a href="https://arxiv.org/abs/2307.08699">Paper </a> |
                  <a href="https://github.com/king159/Pair-Net">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="">
                  <papertitle>Rethinking Evaluation Metrics of Open-Vocabulary Segmentation.</papertitle>
                  </a>
                  <br>
                      Hao Zhou, Tiancheng Shen, Xu Yang, Hai Huang, <strong>Xiangtai Li</strong>, Lu Qi, Ming-Hsuan Yang
                      <br>
                  In: <em>Arxiv (In Submission) </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2311.03352">Arxiv </a> |
                  <a href="https://github.com/qqlu/Entity">Project</a></li>
                  <p> </p>


                    <li>
                  <a href="">
                  <papertitle>Effective Adapter for Face Recognition in the Wild.</papertitle>
                  </a>
                  <br>
                     Yunhao Liu, Lu Qi, Yu-Ju Tsai,  <strong>Xiangtai Li</strong>, Kelvin C. K. Chan, Ming-Hsuan Yang
                      <br>
                  In: <em>Arxiv (In Submission) </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2312.01734">Arxiv </a> |
                  <a href="https://github.com/liuyunhaozz/FaceAdapter/">Project</a></li>
                  <p> </p>





                  <li>
                  <a href="">
                  <papertitle>MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation.
</papertitle>
                  </a>
                  <br>
                    Jiahao Xie, Wei Li, <strong> Xiangtai Li </strong>, Ziwei Liu, Yew Soon Ong, Chen Change Loy
                  <br>
                  In: <em>IJCV (major) </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2309.13042">Arxiv </a> |
                  <a href="https://github.com/Jiahao000/MosaicFusion">Project</a></li>
                  <p> </p>


                  <li>
                  <a href="">
                  <papertitle>Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly Detection.</papertitle>
                  </a>
                  <br>
                    Jiangning Zhang, Xuhai Chen, Yabiao Wang, Chengjie Wang, Yong Liu, <strong> Xiangtai Li</strong> , Ming-Hsuan Yang, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission) </em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2312.07495">Arxiv </a> |
                  <a href="https://zhangzjn.github.io/projects/ViTAD">Project</a></li>
                  <p> </p>




                  <li>
                  <a href="https://arxiv.org/abs/2301.01156">
                  <papertitle>Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation.</papertitle>
                  </a>
                  <br>
                  <a class="php"> Yue Han</a>, Jiangning Zhang, Zhucun Xue, Chao Xu, Xintian Shen, Yabiao Wang, Chengjie Wang, Yong Liu, <strong>Xiangtai Li</strong>
                  <br>
                  In: <em>T-PAMI (major)</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.01156">Arxiv </a> |
                  <a href="https://github.com/hanyue1648/RefT">Code</a></li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2301.01156">
                  <papertitle>Change Detection Methods for Remote Sensing in the Last Decade: A Comprehensive Review.</papertitle>
                  </a>
                  <br>
                      Guangliang Cheng, Yunmeng Huang, <strong>Xiangtai Li</strong>, Shuchang Lyu, Zhaoyang Xu, Qi Zhao, Shiming Xiang
                  <br>
                  In: <em>Arxiv (technical report)</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.01156">Arxiv </a>
                  <p> </p>

                </ol>

                <heading>2022</heading>
                <ol>

                  <li>
                  <a href="https://arxiv.org/abs/2201.05047">
                  <papertitle>TransVOD: End-to-End Video Object Detection with Spatial-Temporal Transformers.</papertitle>
                  </a>
                  <br>
                   <a class="php">Qianyu Zhou*</a>, <strong>Xiangtai Li*</strong>, Lu He, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lizhuang Ma, Dacheng Tao
                  <br>
                  In: <em>IEEE-T-PAMI</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2201.05047">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2107.13155">
                  <papertitle>Improving Video Instance Segmentation via Temporal Pyramid Routing.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Hao He, Yibo Yang, Henghui Ding, Kuiyuan Yang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>IEEE-T-PAMI</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2107.13155">Arxiv</a> |
                  <a href="https://github.com/lxtGH/TemporalPyramidRouting">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2204.04656">
                  <papertitle>Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li*</strong>, Wenwei Zhang*, Jiangmiao Pang*, Kai Chen, Guangliang Cheng, Yunhai Tong, Chen Change Loy
                  <br>
                  In: <em>CVPR</em>, 2022.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2204.04656">Arxiv</a> |
                  <a href="https://github.com/lxtGH/Video-K-Net">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2204.04655">
                  <papertitle>Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Shilin Xu, Yibo Yang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2204.04655">Arxiv</a> |
                  <a href="https://github.com/lxtGH/Panoptic-PartFormer">Code</a> |
                  </li>

                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2204.04654">
                  <papertitle>Fashionformer: A Simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition.</papertitle>
                  </a>
                  <br>
                    <a class="php">Shilin Xu*</a>, <strong>Xiangtai Li*</strong>, Jingbo Wang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2204.04654">Arxiv</a> |
                  <a href="https://github.com/xushilin1/FashionFormer">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2112.02582">
                  <papertitle>PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation.</papertitle>
                  </a>
                  <br>
                   <a class="php">Haobo Yuan*</a>, <strong>Xiangtai Li*</strong>, Yibo Yang, Guangliang Cheng, Jing Zhang, Yunhai Tong, Lefei Zhang, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2112.02582">Arxiv</a> |
                  <a href="https://github.com/HarborYuan/PolyphonicFormer">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2203.09081">
                  <papertitle>Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?</papertitle>
                  </a>
                  <br>
                     Yibo Yang, Shixiang Chen, <strong>Xiangtai Li</strong>, Liang Xie, Zhouchen Lin, Dacheng Tao
                  <br>
                  In: <em>NeurIPS</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2203.09081">Arxiv</a> |
                  <a href="https://github.com/NeuralCollapseApplications/ImbalancedLearning">Code</a> |
                  </li>
                  <p> </p>






                  <li>
                  <a href="https://arxiv.org/abs/2212.08330">
                  <papertitle>Convolution-enhanced Evolving Attention Networks.</papertitle>
                  </a>
                  <br>
                    Yujing Wang, Yaming Yang, Zhuo Li, Jiangang Bai, Mingliang Zhang, <strong>Xiangtai Li</strong>, Jing Yu, Ce Zhang, Gao Huang, Yunhai Tong
                  <br>
                  In: <em>IEEE-T-PAMI</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2212.08330"></a> Paper |
                  <a href="https://github.com/pkuyym/EvolvingAttention">Code</a> |
                  </li>
                  <p> </p>




                </ol>

                <heading>2021</heading>
                <ol>
                  <li>
                  <a href="https://arxiv.org/abs/2105.10920">
                  <papertitle>End-to-End Video Object Detection with Spatial-Temporal Transformers.</papertitle>
                  </a>
                  <br>  <a class="php">Lu He*</a>, Qianyu Zhou*, <strong>Xiangtai Li*</strong>, Li Niu, Guangliang Cheng, Xiao Li, Wenxuan Liu, Yunhai Tong, Lizhuang Ma, Liqing Zhang  <br>
                  In: <em>ACM-MM</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.10920">Arxiv</a> |
                  <a href="https://github.com/SJTU-LuHe/TransVOD">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2105.11651">
                  <papertitle>Fast and Accurate Scene Parsing via Bi-direction Alignment Networks.</papertitle>
                  </a>
                   <br>  <a class="php">Yanran Wu</a>,  <strong>Xiangtai Li</strong>, Chen Shi, Yunhai Tong, Yang Hua, Tao Song, Ruhui Ma, Haibing Guan  <br>
                  In: <em>ICIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11651">Arxiv</a> |
                  <a href="https://github.com/jojacola/BiAlignNet">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2105.11657">
                  <papertitle>Dynamic Dual Sampling Module for Fine-Grained Semantic Segmentation.</papertitle>
                  </a>
                        <br>  <a class="php">Chen Shi</a>, <strong>Xiangtai Li</strong>, Yanran Wu, Yunhai Tong, Yi Xu  <br>
                  In: <em>ICIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11657">Arxiv</a> |
                  <a href="https://github.com/Fantasticarl/DDSM">Code</a> </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2105.11668">
                  <papertitle>BoundarySqueeze: Image Segmentation as Boundary Squeezing.</papertitle>
                  </a>
                  <br>  <a class="php">Hao He*</a>, <strong>Xiangtai Li*</strong>, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lubin Weng, Shiming Xiang, Dacheng Tao  <br>
                  In: <em>Arxiv (in submission)</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11668">Arxiv</a> |
                  <a href="https://github.com/lxtGH/BSSeg">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2103.15734">
                  <papertitle>Enhanced Boundary Learning for Glass-like Object Segmentation.</papertitle>
                  </a>
                    <br>  <a class="php">Hao He*</a>, <strong>Xiangtai Li*</strong>, Guangliang Cheng, Jianping Shi, Yunhai Tong, Gaofeng Meng, Veronique Prinet, Lubin Weng  <br>
                  In: <em>ICCV</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.15734">Arxiv</a> |
                  <a href="https://github.com/hehao13/EBLNet">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2103.06564">
                  <papertitle>PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation.</papertitle>
                  </a>
                  <br>   <strong>Xiangtai Li</strong>*, Hao He*, Xia Li, Duo Li, Guangliang Cheng, Jianping Shi, Lubin Weng, Yunhai Tong, Zhouchen Lin  <br>
                  In: <em>CVPR</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.06564">Arxiv</a> |
                  <a href="https://github.com/lxtGH/PFSegNets">Code</a> |
                  <a href="https://github.com/Jittor/PFSegNets-Jittor">Jitter Code</a>
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2103.06255">
                  <papertitle>Involution: Inverting the Inherence of Convolution for Visual Recognition.</papertitle>
                  </a>
                  <br>  Duo Li, Jie Hu, Changhu Wang, <strong>Xiangtai Li</strong>, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen  <br>
                  In: <em>CVPR</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.06255">Arxiv</a> |
                  <a href="https://github.com/d-li14/involution">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2011.03308">
                  <papertitle>Towards Efficient Scene Understanding via Squeeze Reasoning.</papertitle>
                  </a>
                  <br>  <strong>Xiangtai Li</strong>, Xia Li, Ansheng You, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Zhouchen Lin <br>
                  In: <em>IEEE-TIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2011.03308">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2107.13154">
                  <papertitle>Global Aggregation then Local Distribution for Scene Parsing.</papertitle>
                  </a>
                  <br>  <strong>Xiangtai Li</strong>, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Xiatian Zhu, Tao Xiang <br>
                  In: <em>IEEE-TIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2107.13154">Arxiv</a> |
                  <a href="https://github.com/lxtGH/GALD-DGCNet">Code</a> </li>
                  <p> </p>

                </ol>


                <heading>2020</heading>
                <ol>
                    <li>
                  <a href="https://arxiv.org/abs/2002.10120">
                  <papertitle>Semantic Flow for Fast and Accurate Scene Parsing.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Ansheng You, Zhen Zhu, Houlong Zhao, Maoke Yang, Kuiyuan Yang, Yunhai Tong
                  <br>
                  In: <em>ECCV</em>, 2020.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2002.10120">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> |
                        <a href="https://github.com/donnyyou/torchcv">Code (torchcv)</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2007.10035">
                  <papertitle>Improving Semantic Segmentation via Decoupled Body and Edge Supervision.</papertitle>
                  </a>
                  <br>
                   <strong>Xiangtai Li </strong>, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, Yunhai Tong
                  <br>
                  In: <em>ECCV</em>, 2020.
                  <br>
                  <a href="https://arxiv.org/abs/2007.10035">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/1904.01803">
                  <papertitle>GFF: Gated Fully Fusion for Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Houlong Zhao, Lei Han, Yunhai Tong, Kuiyuan Yang
                  <br>
                  In: <em>AAAI</em>, 2020.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/1904.01803">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a> |
                  </li>
                  <p> </p>

                </ol>


                <heading>2019</heading>
                <ol>
                  <li>
                  <a href="https://arxiv.org/abs/1909.07229">
                  <papertitle>Global Aggregation then Local Distribution in Fully Convolutional Networks.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai Tong
                  <br>
                  In: <em>BMVC</em>, 2019.
                  <a href="https://arxiv.org/abs/1909.07229">Paper</a> |
                  <a href="hhttps://github.com/lxtGH/GALD-DGCNet">Code</a> |
                  <p> </p>

                    <li>
                  <a href="https://arxiv.org/abs/1909.06121">
                  <papertitle>Dual Graph Convolutional Network for Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  Li Zhang*, <strong>Xiangtai Li*</strong>, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, Philip H.S. Torr
                  <br>
                  In: <em>BMVC</em>, 2019.
                  <a href="https://arxiv.org/abs/1909.06121">Paper</a> |
                  <a href="hhttps://github.com/lxtGH/GALD-DGCNet">Code</a> |
                  <p> </p>

                    <li>
                  <a href="../paper_local/Flow2Seg_motion.pdf">
                  <papertitle>Flo2seg: Motion-Aided Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Jiangang Bai, Yunhai Tong, Kuiyuan Yang
                  <br>
                  In: <em>IJCNN</em>, 2019. long paper
                  <a href="../paper_local/Flow2Seg_motion.pdf">Paper</a> |
                  <p> </p>

                </ol>
              </td>
            </tr>
            </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                      <td>
                        <br>
                        <p align="center">
                          <font size="2">

                            Based on a template by <a href="https://jonbarron.info">Jon Barron</a>

                            </font>
                        </p>
                      </td>
                    </tr>
                  </table>
                  </td>
              </tr>
            </table>
          </body>
          </html>