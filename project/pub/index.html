
<!DOCTYPE HTML>
<html lang="en">

<body>
  <table id="container">
    <tr>
      <td>
         * denotes equal contribution
        <table width="130%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>2023</heading>
                <ol>

                  <li>
                  <a href="https://arxiv.org/abs/2301.01156">
                  <papertitle>Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation.</papertitle>
                  </a>
                  <br>
                  Yue Han, Jiangning Zhang, Zhucun Xue, Chao Xu, Xintian Shen, Yabiao Wang, Chengjie Wang, Yong Liu, <strong>Xiangtai Li</strong>
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.01156">Arxiv </a> |
                  <a href="https://github.com/hanyue1648/RefT">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2301.01146">
                  <papertitle>Rethinking Mobile Block for Efficient Neural Models.</papertitle>
                  </a>
                  <br>
                    Jiangning Zhang, <strong>Xiangtai Li</strong>, Jian Li, Liang Liu, Zhucun Xue, Boshen Zhang, Zhengkai Jiang, Tianxin Huang, Yabiao Wang, Chengjie Wang
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/abs/2301.01146">Arxiv</a> |
                  <a href="https://github.com/zhangzjn/EMO">Code</a> |
                  </li>
                  <p> </p>


                </ol>

                <heading>2022</heading>
                <ol>

                  <li>
                  <a href="https://arxiv.org/abs/2201.05047">
                  <papertitle>TransVOD: End-to-End Video Object Detection with Spatial-Temporal Transformers.</papertitle>
                  </a>
                  <br>
                   Qianyu Zhou*, <strong>Xiangtai Li*</strong>, Lu He, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lizhuang Ma, Dacheng Tao
                  <br>
                  In: <em>IEEE-T-PAMI</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2201.05047">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2107.13155">
                  <papertitle>Improving Video Instance Segmentation via Temporal Pyramid Routing.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Hao He, Yibo Yang, Henghui Ding, Kuiyuan Yang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>IEEE-T-PAMI</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2107.13155">Arxiv</a> |
                  <a href="https://github.com/lxtGH/TemporalPyramidRouting">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2204.04656">
                  <papertitle>Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li*</strong>, Wenwei Zhang*, Jiangmiao Pang*, Kai Chen, Guangliang Cheng, Yunhai Tong, Chen Change Loy
                  <br>
                  In: <em>CVPR</em>, 2022.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2204.04656">Arxiv</a> |
                  <a href="https://github.com/lxtGH/Video-K-Net">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2204.04655">
                  <papertitle>Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Shilin Xu, Yibo Yang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2204.04655">Arxiv</a> |
                  <a href="https://github.com/lxtGH/Panoptic-PartFormer">Code</a> |
                  </li>

                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2204.04654">
                  <papertitle>Fashionformer: A Simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition.</papertitle>
                  </a>
                  <br>
                    Shilin Xu*, <strong>Xiangtai Li*</strong>, Jingbo Wang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2204.04654">Arxiv</a> |
                  <a href="https://github.com/xushilin1/FashionFormer">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2112.02582">
                  <papertitle>PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation.</papertitle>
                  </a>
                  <br>
                   Haobo Yuan*, <strong>Xiangtai Li*</strong>, Yibo Yang, Guangliang Cheng, Jing Zhang, Yunhai Tong, Lefei Zhang, Dacheng Tao
                  <br>
                  In: <em>ECCV</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2112.02582">Arxiv</a> |
                  <a href="https://github.com/HarborYuan/PolyphonicFormer">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2203.09081">
                  <papertitle>Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?</papertitle>
                  </a>
                  <br>
                     Yibo Yang, Shixiang Chen, <strong>Xiangtai Li</strong>, Liang Xie, Zhouchen Lin, Dacheng Tao
                  <br>
                  In: <em>NeurIPS</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2203.09081">Arxiv</a> |
                  <a href="https://github.com/NeuralCollapseApplications/ImbalancedLearning">Code</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2205.14354">
                  <papertitle>Multi-Task Learning with Multi-Query Transformer for Dense Prediction.</papertitle>
                  </a>
                  <br>
                    Yangyang Xu*, <strong>Xiangtai Li*</strong>, Haobo Yuan, Yibo Yang, Jing Zhang, Yunhai Tong, Lefei Zhang, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2205.14354">Arxiv</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2206.09325">
                  <papertitle>EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm.</papertitle>
                  </a>
                  <br>
                  Jiangning Zhang*, <strong>Xiangtai Li*</strong>, Yabiao Wang, Chengjie Wang, Yibo Yang, Yong Liu, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2206.09325">Arxiv</a> |
                  <a href="https://https://github.com/zhangzjn/EATFormer">Code</a> |
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2207.04415">
                  <papertitle>SFNet: Faster, Accurate, and Domain Agnostic Semantic Segmentation via Semantic Flow.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Jiangning Zhang, Yibo Yang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2207.04415">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2209.09554">
                  <papertitle>Towards Robust Referring Image Segmentation.</papertitle>
                  </a>
                  <br>
                  Jianzong Wu*, <strong>Xiangtai Li*</strong>, Xia Li, Henghui Ding, Yunhai Tong, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2209.09554">Arxiv</a> |
                  <a href="https://lxtgh.github.io/project/robust_ref_seg/">Code</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2212.08330">
                  <papertitle>Convolution-enhanced Evolving Attention Networks.</papertitle>
                  </a>
                  <br>
                    Yujing Wang, Yaming Yang, Zhuo Li, Jiangang Bai, Mingliang Zhang, <strong>Xiangtai Li</strong>, Jing Yu, Ce Zhang, Gao Huang, Yunhai Tong
                  <br>
                  In: <em>T-PAMI-2023 (minor)</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2212.08330"></a> Paper |
                  <a href="https://github.com/pkuyym/EvolvingAttention">Code</a> |
                  </li>
                  <p> </p>

                    <li>
                  <a href="https://arxiv.org/abs/2301.00954">
                  <papertitle>PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation. </papertitle>
                  </a>
                  <br>
                        <strong>Xiangtai Li </strong>, Shilin Xu, Yibo Yang, Haobo Yuan, Guangliang Cheng, Yunhai Tong, Zhouchen Lin, Dacheng Tao
                  <br>
                  In: <em>Arxiv (In Submission), Extension of PanopticPartFormer</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2301.00954"></a> Paper |
                  <a href="https://github.com/lxtGH/Panoptic-PartFormer">Code</a> |
                  </li>
                  <p> </p>

                    <li>
                  <a href="https://arxiv.org/abs/2301.00805">
                  <papertitle>Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation.</papertitle>
                  </a>
                  <br>
                    Jianzong Wu*, <strong>Xiangtai Li*</strong>, Henghui Ding, Xia Li, Guangliang Cheng, Yunhai Tong, Chen Change Loy
                  <br>
                  In: <em>Arxiv (In Submission)</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2301.00805"></a> Paper |
                  <a href="https://github.com/jzwu48033552/betrayed-by-captions">Code</a> |
                  </li>
                  <p> </p>

                </ol>

                <heading>2021</heading>
                <ol>
                  <li>
                  <a href="https://arxiv.org/abs/2105.10920">
                  <papertitle>End-to-End Video Object Detection with Spatial-Temporal Transformers.</papertitle>
                  </a>
                  <br>  Lu He*, Qianyu Zhou*, <strong>Xiangtai Li*</strong>, Li Niu, Guangliang Cheng, Xiao Li, Wenxuan Liu, Yunhai Tong, Lizhuang Ma, Liqing Zhang  <br>
                  In: <em>ACM-MM</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.10920">Arxiv</a> |
                  <a href="https://github.com/SJTU-LuHe/TransVOD">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2105.11651">
                  <papertitle>Fast and Accurate Scene Parsing via Bi-direction Alignment Networks.</papertitle>
                  </a>
                  <br>  Chen Shi, <strong>Xiangtai Li</strong>, Yanran Wu, Yunhai Tong, Yi Xu  <br>
                  In: <em>ICIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11651">Arxiv</a> |
                  <a href="https://github.com/jojacola/BiAlignNet">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2105.11657">
                  <papertitle>Dynamic Dual Sampling Module for Fine-Grained Semantic Segmentation.</papertitle>
                  </a>
                  <br>  Yanran Wu,  <strong>Xiangtai Li</strong>, Chen Shi, Yunhai Tong, Yang Hua, Tao Song, Ruhui Ma, Haibing Guan  <br>
                  In: <em>ICIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11657">Arxiv</a> |
                  <a href="https://github.com/Fantasticarl/DDSM">Code</a> </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2105.11668">
                  <papertitle>BoundarySqueeze: Image Segmentation as Boundary Squeezing.</papertitle>
                  </a>
                  <br>  Hao He*, <strong>Xiangtai Li*</strong>, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lubin Weng, Shiming Xiang, Dacheng Tao  <br>
                  In: <em>Arxiv (in submission)</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2105.11668">Arxiv</a> |
                  <a href="https://github.com/lxtGH/BSSeg">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2103.15734">
                  <papertitle>Enhanced Boundary Learning for Glass-like Object Segmentation.</papertitle>
                  </a>
                  <br>  Hao He*, <strong>Xiangtai Li*</strong>, Guangliang Cheng, Jianping Shi, Yunhai Tong, Gaofeng Meng, Veronique Prinet, Lubin Weng  <br>
                  In: <em>ICCV</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.15734">Arxiv</a> |
                  <a href="https://github.com/hehao13/EBLNet">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2103.06564">
                  <papertitle>PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation.</papertitle>
                  </a>
                  <br>   <strong>Xiangtai Li</strong>*, Hao He*, Xia Li, Duo Li, Guangliang Cheng, Jianping Shi, Lubin Weng, Yunhai Tong, Zhouchen Lin  <br>
                  In: <em>CVPR</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.06564">Arxiv</a> |
                  <a href="https://github.com/lxtGH/PFSegNets">Code</a> |
                  <a href="https://github.com/Jittor/PFSegNets-Jittor">Jitter Code</a>
                  </li>
                  <p> </p>


                  <li>
                  <a href="https://arxiv.org/abs/2103.06255">
                  <papertitle>Involution: Inverting the Inherence of Convolution for Visual Recognition.</papertitle>
                  </a>
                  <br>  Duo Li, Jie Hu, Changhu Wang, <strong>Xiangtai Li</strong>, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen  <br>
                  In: <em>CVPR</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2103.06255">Arxiv</a> |
                  <a href="https://github.com/d-li14/involution">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2011.03308">
                  <papertitle>Towards Efficient Scene Understanding via Squeeze Reasoning.</papertitle>
                  </a>
                  <br>  <strong>Xiangtai Li</strong>, Xia Li, Ansheng You, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Zhouchen Lin <br>
                  In: <em>IEEE-TIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2011.03308">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2107.13154">
                  <papertitle>Global Aggregation then Local Distribution for Scene Parsing.</papertitle>
                  </a>
                  <br>  <strong>Xiangtai Li</strong>, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Xiatian Zhu, Tao Xiang <br>
                  In: <em>IEEE-TIP</em>, 2021.<br>
                  <a href="https://arxiv.org/abs/2107.13154">Arxiv</a> |
                  <a href="https://github.com/lxtGH/GALD-DGCNet">Code</a> </li>
                  <p> </p>

                </ol>


                <heading>2020</heading>
                <ol>
                    <li>
                  <a href="https://arxiv.org/abs/2002.10120">
                  <papertitle>Semantic Flow for Fast and Accurate Scene Parsing.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Ansheng You, Zhen Zhu, Houlong Zhao, Maoke Yang, Kuiyuan Yang, Yunhai Tong
                  <br>
                  In: <em>ECCV</em>, 2020.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2002.10120">Arxiv</a> |
                  <a href="https://github.com/lxtGH/SFSegNets">Code</a> |
                        <a href="https://github.com/donnyyou/torchcv">Code (torchcv)</a> |
                  </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2007.10035">
                  <papertitle>Improving Semantic Segmentation via Decoupled Body and Edge Supervision.</papertitle>
                  </a>
                  <br>
                   <strong>Xiangtai Li </strong>, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, Yunhai Tong
                  <br>
                  In: <em>ECCV</em>, 2020.
                  <br>
                  <a href="https://arxiv.org/abs/2007.10035">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/1904.01803">
                  <papertitle>GFF: Gated Fully Fusion for Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Houlong Zhao, Lei Han, Yunhai Tong, Kuiyuan Yang
                  <br>
                  In: <em>AAAI</em>, 2020.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/1904.01803">Arxiv</a> |
                  <a href="https://github.com/lxtGH/DecoupleSegNets">Code</a> |
                  </li>
                  <p> </p>

                </ol>


                <heading>2019</heading>
                <ol>
                  <li>
                  <a href="https://arxiv.org/abs/1909.07229">
                  <papertitle>Global Aggregation then Local Distribution in Fully Convolutional Networks.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai Tong
                  <br>
                  In: <em>BMVC</em>, 2019.
                  <a href="https://arxiv.org/abs/1909.07229">Paper</a> |
                  <a href="hhttps://github.com/lxtGH/GALD-DGCNet">Code</a> |
                  <p> </p>

                    <li>
                  <a href="https://arxiv.org/abs/1909.06121">
                  <papertitle>Dual Graph Convolutional Network for Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  Li Zhang*, <strong>Xiangtai Li*</strong>, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, Philip H.S. Torr
                  <br>
                  In: <em>BMVC</em>, 2019.
                  <a href="https://arxiv.org/abs/1909.06121">Paper</a> |
                  <a href="hhttps://github.com/lxtGH/GALD-DGCNet">Code</a> |
                  <p> </p>

                    <li>
                  <a href="../paper_local/Flow2Seg_motion.pdf">
                  <papertitle>Flo2seg: Motion-Aided Semantic Segmentation.</papertitle>
                  </a>
                  <br>
                  <strong>Xiangtai Li</strong>, Jiangang Bai, Yunhai Tong, Kuiyuan Yang
                  <br>
                  In: <em>IJCNN</em>, 2019. long paper
                  <a href="../paper_local/Flow2Seg_motion.pdf">Paper</a> |
                  <p> </p>

                </ol>
              </td>
            </tr>
            </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                      <td>
                        <br>
                        <p align="center">
                          <font size="2">
                            Based on a template by <a href="https://jonbarron.info">Jon Barron</a>
                            </font>
                        </p>
                      </td>
                    </tr>
                  </table>
                  </td>
              </tr>
            </table>
          </body>
          </html>