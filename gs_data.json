{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "FL3ReD0AAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Xiangtai Li", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=FL3ReD0AAAAJ&citpid=5", "affiliation": "Research Scientist, Bytedance Inc. (Tiktok) ; MMLab@NTU; PKU", "organization": 10725744176602846184, "interests": ["Generative AI", "Multi-modal", "Computer Vision"], "email_domain": "@pku.edu.cn", "homepage": "https://lxtgh.github.io/", "citedby": 9043, "publications": {"FL3ReD0AAAAJ:KlAtU1dfN6UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Semantic flow for fast and accurate scene parsing", "pub_year": "2020"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:KlAtU1dfN6UC", "num_citations": 630, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6758824951435280336,18278637230540179580,8245723423576086879", "cites_id": ["6758824951435280336", "18278637230540179580", "8245723423576086879"]}, "FL3ReD0AAAAJ:eQOLeE2rZwMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Involution: Inverting the Inherence of Convolution for Visual Recognition", "pub_year": "2021"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:eQOLeE2rZwMC", "num_citations": 546, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=393441868853886664,13354345524273744787,8726808876409056888,15644658627305340607,15547244705994340753", "cites_id": ["393441868853886664", "13354345524273744787", "8726808876409056888", "15644658627305340607", "15547244705994340753"]}, "FL3ReD0AAAAJ:AvfA0Oy_GE0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:AvfA0Oy_GE0C", "num_citations": 494, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11972440028159904664", "cites_id": ["11972440028159904664"]}, "FL3ReD0AAAAJ:NJ774b8OgUMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Improving semantic segmentation via decoupled body and edge supervision", "pub_year": "2020"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:NJ774b8OgUMC", "num_citations": 404, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17990846409743098772,4164997443925569291,4547548012652392544,10275861266495162914", "cites_id": ["17990846409743098772", "4164997443925569291", "4547548012652392544", "10275861266495162914"]}, "FL3ReD0AAAAJ:k_IJM867U9cC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Rethinking mobile block for efficient neural models", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:k_IJM867U9cC", "num_citations": 359, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8399801815353872086,6803692528838506853,4462447219040254297,17660248115421671253,10762965302185464459,863325303809913768", "cites_id": ["8399801815353872086", "6803692528838506853", "4462447219040254297", "17660248115421671253", "10762965302185464459", "863325303809913768"]}, "FL3ReD0AAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Dual graph convolutional network for semantic segmentation", "pub_year": "2019"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:qjMakFHDy7sC", "num_citations": 303, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8974784740086755370", "cites_id": ["8974784740086755370"]}, "FL3ReD0AAAAJ:GnPB-g6toBAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Transformer-based visual segmentation: A survey", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:GnPB-g6toBAC", "num_citations": 300, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1538484425142436735", "cites_id": ["1538484425142436735"]}, "FL3ReD0AAAAJ:roLk4NBRz8UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Gated Fully Fusion for Semantic Segmentation", "pub_year": "2020"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:roLk4NBRz8UC", "num_citations": 277, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4471456527181792079", "cites_id": ["4471456527181792079"]}, "FL3ReD0AAAAJ:70eg2SAEIzsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Towards Open Vocabulary Learning: A Survey", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:70eg2SAEIzsC", "num_citations": 256, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4374069864889678520,11787347144589844739", "cites_id": ["4374069864889678520", "11787347144589844739"]}, "FL3ReD0AAAAJ:JV2RwH3_ST0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class Incremental Learning", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:JV2RwH3_ST0C", "num_citations": 242, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2479614036110317497", "cites_id": ["2479614036110317497"]}, "FL3ReD0AAAAJ:ns9cj8rnVeAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Change detection methods for remote sensing in the last decade: A comprehensive review", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:ns9cj8rnVeAC", "num_citations": 233, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17545616755808139020,3671990082268910787", "cites_id": ["17545616755808139020", "3671990082268910787"]}, "FL3ReD0AAAAJ:M05iB0D1s5AC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "TransVOD: end-to-end video object detection with spatial-temporal transformers", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:M05iB0D1s5AC", "num_citations": 230, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=18249743012593625562", "cites_id": ["18249743012593625562"]}, "FL3ReD0AAAAJ:TQgYirikUcIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?", "pub_year": "2022"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:TQgYirikUcIC", "num_citations": 196, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16056665854734462164", "cites_id": ["16056665854734462164"]}, "FL3ReD0AAAAJ:6ZxmRoH8BuwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Point cloud mamba: Point cloud learning via state space model", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:6ZxmRoH8BuwC", "num_citations": 182, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11650303125677654154,1066942803128278898", "cites_id": ["11650303125677654154", "1066942803128278898"]}, "FL3ReD0AAAAJ:uLbwQdceFCQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:uLbwQdceFCQC", "num_citations": 179, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13714848949410671995", "cites_id": ["13714848949410671995"]}, "FL3ReD0AAAAJ:Y5dfb0dijaUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mambaad: Exploring state space models for multi-class unsupervised anomaly detection", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:Y5dfb0dijaUC", "num_citations": 168, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15481876724486702892", "cites_id": ["15481876724486702892"]}, "FL3ReD0AAAAJ:WF5omc3nYNoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation", "pub_year": "2021"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:WF5omc3nYNoC", "num_citations": 148, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9470013847827825734", "cites_id": ["9470013847827825734"]}, "FL3ReD0AAAAJ:LkGwnXOMwfcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Enhanced Boundary Learning for Glass-like Object Segmentation", "pub_year": "2021"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:LkGwnXOMwfcC", "num_citations": 137, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9868407404558335342,4251627827542796547", "cites_id": ["9868407404558335342", "4251627827542796547"]}, "FL3ReD0AAAAJ:ldfaerwXgEUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "End-to-end video object detection with spatial-temporal transformers", "pub_year": "2021"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:ldfaerwXgEUC", "num_citations": 136, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4349671636320753508", "cites_id": ["4349671636320753508"]}, "FL3ReD0AAAAJ:9ZlFYXVOiuMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation", "pub_year": "2022"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:9ZlFYXVOiuMC", "num_citations": 132, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17741523309785170156", "cites_id": ["17741523309785170156"]}, "FL3ReD0AAAAJ:1yQoGdGgb4wC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:1yQoGdGgb4wC", "num_citations": 127, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13032522514043309996,795606992797610296", "cites_id": ["13032522514043309996", "795606992797610296"]}, "FL3ReD0AAAAJ:738O_yMBCRsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "OMG-Seg: Is One Model Good Enough For All Segmentation?", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:738O_yMBCRsC", "num_citations": 123, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4364839080600440600", "cites_id": ["4364839080600440600"]}, "FL3ReD0AAAAJ:cFHS6HbyZ2cC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Clipself: Vision transformer distills itself for open-vocabulary dense prediction", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:cFHS6HbyZ2cC", "num_citations": 121, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9286809064946144834", "cites_id": ["9286809064946144834"]}, "FL3ReD0AAAAJ:PR6Y55bgFSsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Xtuner: A toolkit for efficiently fine-tuning llm", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:PR6Y55bgFSsC", "num_citations": 109, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9195622785808607295", "cites_id": ["9195622785808607295"]}, "FL3ReD0AAAAJ:Tiz5es2fbqcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Open-vocabulary SAM: Segment and recognize twenty-thousand classes interactively", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:Tiz5es2fbqcC", "num_citations": 109, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15705460866819792681,7088044729516783237,4993548792865099774,9858564583378160570", "cites_id": ["15705460866819792681", "7088044729516783237", "4993548792865099774", "9858564583378160570"]}, "FL3ReD0AAAAJ:U4n9YNQMCAIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Rtmo: Towards high-performance one-stage real-time multi-person pose estimation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:U4n9YNQMCAIC", "num_citations": 106, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11852186620874685917", "cites_id": ["11852186620874685917"]}, "FL3ReD0AAAAJ:MXK_kJrjxJIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Global aggregation then local distribution for scene parsing", "pub_year": "2021"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:MXK_kJrjxJIC", "num_citations": 105, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6892749512101145310,18236384371374137173", "cites_id": ["6892749512101145310", "18236384371374137173"]}, "FL3ReD0AAAAJ:-f6ydRqryjwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Towards Robust Referring Image Segmentation", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:-f6ydRqryjwC", "num_citations": 101, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2404495366684386735", "cites_id": ["2404495366684386735"]}, "FL3ReD0AAAAJ:KxtntwgDAa4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Edgesam: Prompt-in-the-loop distillation for on-device deployment of sam", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:KxtntwgDAa4C", "num_citations": 94, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17706561528705038380,607257711798200117,15127962014868286078", "cites_id": ["17706561528705038380", "607257711798200117", "15127962014868286078"]}, "FL3ReD0AAAAJ:L8Ckcad2t8MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Multi-Task Learning with Multi-query Transformer for Dense Prediction", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:L8Ckcad2t8MC", "num_citations": 87, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8246617585384770274", "cites_id": ["8246617585384770274"]}, "FL3ReD0AAAAJ:tuHXwOkdijsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:tuHXwOkdijsC", "num_citations": 85, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4655890432296464228", "cites_id": ["4655890432296464228"]}, "FL3ReD0AAAAJ:VaXvl8Fpj5cC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Exploring plain ViT features for multi-class unsupervised visual anomaly detection", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:VaXvl8Fpj5cC", "num_citations": 82, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1180170355200597287,1075116936351398361", "cites_id": ["1180170355200597287", "1075116936351398361"]}, "FL3ReD0AAAAJ:UHK10RUVsp4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:UHK10RUVsp4C", "num_citations": 81, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5217561798098030587", "cites_id": ["5217561798098030587"]}, "FL3ReD0AAAAJ:hC7cP41nSMkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Sfnet: faster and accurate semantic segmentation via semantic flow", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:hC7cP41nSMkC", "num_citations": 77, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=423028739487429125,16246269217124681758", "cites_id": ["423028739487429125", "16246269217124681758"]}, "FL3ReD0AAAAJ:NaGl4SEjCO4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Panoptic video scene graph generation", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:NaGl4SEjCO4C", "num_citations": 74, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6906505672540866921", "cites_id": ["6906505672540866921"]}, "FL3ReD0AAAAJ:u9iWguZQMMsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MM-GroundingDINO: An open and comprehensive pipeline for unified object grounding and detection", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:u9iWguZQMMsC", "num_citations": 71, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4008630785504171805", "cites_id": ["4008630785504171805"]}, "FL3ReD0AAAAJ:NMxIlDl6LWMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Tube-link: A flexible cross tube baseline for universal video segmentation", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:NMxIlDl6LWMC", "num_citations": 70, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=420973666035668953", "cites_id": ["420973666035668953"]}, "FL3ReD0AAAAJ:SdhP9T11ey4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:SdhP9T11ey4C", "num_citations": 67, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17368182402162268103", "cites_id": ["17368182402162268103"]}, "FL3ReD0AAAAJ:L7CI7m0gUJcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:L7CI7m0gUJcC", "num_citations": 65, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16430844593300956743", "cites_id": ["16430844593300956743"]}, "FL3ReD0AAAAJ:ZeXyd9-uunAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Eatformer: Improving vision transformer inspired by evolutionary algorithm", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:ZeXyd9-uunAC", "num_citations": 65, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14151647974405698917,12574250028327747897", "cites_id": ["14151647974405698917", "12574250028327747897"]}, "FL3ReD0AAAAJ:Wp0gIr-vW9MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation", "pub_year": "2022"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:Wp0gIr-vW9MC", "num_citations": 61, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11513198882440237429,2391568422936129146,10633307948345430554", "cites_id": ["11513198882440237429", "2391568422936129146", "10633307948345430554"]}, "FL3ReD0AAAAJ:iH-uZ7U-co4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Convolution-enhanced Evolving Attention Networks", "pub_year": "2022"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:iH-uZ7U-co4C", "num_citations": 60, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10840753293659753469,3044407406470957767", "cites_id": ["10840753293659753469", "3044407406470957767"]}, "FL3ReD0AAAAJ:GtLg2Ama23sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "NTIRE 2025 challenge on day and night raindrop removal for dual-focused images: Methods and results", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:GtLg2Ama23sC", "num_citations": 53, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14445037370129112785,18445670938700027125,2728708782343488457,4402707799563769025", "cites_id": ["14445037370129112785", "18445670938700027125", "2728708782343488457", "4402707799563769025"]}, "FL3ReD0AAAAJ:u_35RYKgDlwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mosaicfusion: Diffusion models as data augmenters for large vocabulary instance segmentation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:u_35RYKgDlwC", "num_citations": 52, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3801142790310250854", "cites_id": ["3801142790310250854"]}, "FL3ReD0AAAAJ:QIV2ME_5wuYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation", "pub_year": "2022"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:QIV2ME_5wuYC", "num_citations": 52, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7127843590064680446,10754828275755348128", "cites_id": ["7127843590064680446", "10754828275755348128"]}, "FL3ReD0AAAAJ:XD-gHx7UXLsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:XD-gHx7UXLsC", "num_citations": 51, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15549031111794762631", "cites_id": ["15549031111794762631"]}, "FL3ReD0AAAAJ:LO7wyVUgiFcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "On Path to Multimodal Generalist: General-Level and General-Bench", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:LO7wyVUgiFcC", "num_citations": 48, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1362657957922559073", "cites_id": ["1362657957922559073"]}, "FL3ReD0AAAAJ:lSLTfruPkqcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Explore In-Context Learning for 3D Point Cloud Understanding", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:lSLTfruPkqcC", "num_citations": 44, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6233865000547078496,6121563124273395159", "cites_id": ["6233865000547078496", "6121563124273395159"]}, "FL3ReD0AAAAJ:mVmsd5A6BfQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Fashionformer: A Simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition", "pub_year": "2022"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:mVmsd5A6BfQC", "num_citations": 42, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14517707013240558642,11009063257060572820", "cites_id": ["14517707013240558642", "11009063257060572820"]}, "FL3ReD0AAAAJ:TFP_iSt0sucC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:TFP_iSt0sucC", "num_citations": 39, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1291513963242025049", "cites_id": ["1291513963242025049"]}, "FL3ReD0AAAAJ:vRqMK49ujn8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Referring Image Editing: Object-level Image Editing via Referring Expressions", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:vRqMK49ujn8C", "num_citations": 37, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7244170885589678862", "cites_id": ["7244170885589678862"]}, "FL3ReD0AAAAJ:0KyAp5RtaNEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "PredFormer: transformers are effective spatial-temporal predictive learners", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:0KyAp5RtaNEC", "num_citations": 36, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5362278884792813492,11714204893967712979", "cites_id": ["5362278884792813492", "11714204893967712979"]}, "FL3ReD0AAAAJ:l7t_Zn2s7bgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Towards Language-Driven Video Inpainting via Multimodal Large Language Models", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:l7t_Zn2s7bgC", "num_citations": 36, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8854937429748086141", "cites_id": ["8854937429748086141"]}, "FL3ReD0AAAAJ:ZfRJV9d4-WMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "PointRWKV: Efficient RWKV-Like Model for Hierarchical Point Cloud Learning", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:ZfRJV9d4-WMC", "num_citations": 35, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11075235487468564374", "cites_id": ["11075235487468564374"]}, "FL3ReD0AAAAJ:g5m5HwL7SMYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Pair then relation: Pair-net for panoptic scene graph generation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:g5m5HwL7SMYC", "num_citations": 34, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16917096055478569447", "cites_id": ["16917096055478569447"]}, "FL3ReD0AAAAJ:JQOojiI6XY0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mamba or rwkv: Exploring high-quality and high-efficiency segment anything model", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:JQOojiI6XY0C", "num_citations": 33, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8856131519264889251", "cites_id": ["8856131519264889251"]}, "FL3ReD0AAAAJ:XiSMed-E-HIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:XiSMed-E-HIC", "num_citations": 33, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9623746480157277861", "cites_id": ["9623746480157277861"]}, "FL3ReD0AAAAJ:CHSYGLWDkRkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Skeleton-in-context: Unified skeleton sequence modeling with in-context learning", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:CHSYGLWDkRkC", "num_citations": 33, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13914761776338681802,1844830638974496595", "cites_id": ["13914761776338681802", "1844830638974496595"]}, "FL3ReD0AAAAJ:PVjk1bu6vJQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:PVjk1bu6vJQC", "num_citations": 31, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1483573169413633987", "cites_id": ["1483573169413633987"]}, "FL3ReD0AAAAJ:isC4tDSrTZIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:isC4tDSrTZIC", "num_citations": 30, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5798636040124543876", "cites_id": ["5798636040124543876"]}, "FL3ReD0AAAAJ:EUQCXRtRnyEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "4D Panoptic Scene Graph Generation", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:EUQCXRtRnyEC", "num_citations": 30, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5387739382498205019", "cites_id": ["5387739382498205019"]}, "FL3ReD0AAAAJ:eMMeJKvmdy0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Dgmamba: Domain generalization via generalized state space model", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:eMMeJKvmdy0C", "num_citations": 29, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1810006389448009171", "cites_id": ["1810006389448009171"]}, "FL3ReD0AAAAJ:eq2jaN3J8jMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:eq2jaN3J8jMC", "num_citations": 28, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11792674059472940322", "cites_id": ["11792674059472940322"]}, "FL3ReD0AAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Towards Efficient Scene Understanding via Squeeze Reasoning", "pub_year": "2021"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:YsMSGLbcyi4C", "num_citations": 28, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16027297481408914642,7029398176905985841,3671826537264544011", "cites_id": ["16027297481408914642", "7029398176905985841", "3671826537264544011"]}, "FL3ReD0AAAAJ:dQ2og3OwTAUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mg-llava: Towards multi-granularity visual instruction tuning", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:dQ2og3OwTAUC", "num_citations": 27, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4248375566981125314", "cites_id": ["4248375566981125314"]}, "FL3ReD0AAAAJ:maZDTaKrznsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "PanopticPartFormer++: A Unified and Decoupled View for Panoptic Part Segmentation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:maZDTaKrznsC", "num_citations": 27, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10115806809001991372", "cites_id": ["10115806809001991372"]}, "FL3ReD0AAAAJ:bz8QjSJIRt4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The scalability of simplicity: Empirical analysis of vision-language learning with a single transformer", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:bz8QjSJIRt4C", "num_citations": 26, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13191182875896936982,16695445559823321133", "cites_id": ["13191182875896936982", "16695445559823321133"]}, "FL3ReD0AAAAJ:vbGhcppDl1QC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Learning feature inversion for multi-class anomaly detection under general-purpose coco-ad benchmark", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:vbGhcppDl1QC", "num_citations": 25, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17322629230626563646,16323182256059246414", "cites_id": ["17322629230626563646", "16323182256059246414"]}, "FL3ReD0AAAAJ:EYYDruWGBe4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:EYYDruWGBe4C", "num_citations": 25, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17674757387173740034", "cites_id": ["17674757387173740034"]}, "FL3ReD0AAAAJ:1qzjygNMrQYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:1qzjygNMrQYC", "num_citations": 25, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7484271781936981758", "cites_id": ["7484271781936981758"]}, "FL3ReD0AAAAJ:a0OBvERweLwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Dst-det: Simple dynamic self-training for open-vocabulary object detection", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:a0OBvERweLwC", "num_citations": 23, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6953300397849811721,2884967132956023818", "cites_id": ["6953300397849811721", "2884967132956023818"]}, "FL3ReD0AAAAJ:XvxMoLDsR5gC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Muddit: Liberating generation beyond text-to-image with a unified discrete diffusion model", "pub_year": "2026"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:XvxMoLDsR5gC", "num_citations": 21, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13322954640730457712,2017078427080073652", "cites_id": ["13322954640730457712", "2017078427080073652"]}, "FL3ReD0AAAAJ:dBIO0h50nwkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology", "pub_year": "2026"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:dBIO0h50nwkC", "num_citations": 19, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16765734953193520", "cites_id": ["16765734953193520"]}, "FL3ReD0AAAAJ:VOx2b1Wkg3QC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Explore in-context segmentation via latent diffusion models", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:VOx2b1Wkg3QC", "num_citations": 19, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11945616096981116533", "cites_id": ["11945616096981116533"]}, "FL3ReD0AAAAJ:f2IySw72cVMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Ov-vg: A benchmark for open-vocabulary visual grounding", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:f2IySw72cVMC", "num_citations": 19, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1380029128233384113", "cites_id": ["1380029128233384113"]}, "FL3ReD0AAAAJ:LhH-TYMQEocC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:LhH-TYMQEocC", "num_citations": 18, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8830646779127013911", "cites_id": ["8830646779127013911"]}, "FL3ReD0AAAAJ:S16KYo8Pm5AC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:S16KYo8Pm5AC", "num_citations": 18, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1232878462404916650,18148123035081125714", "cites_id": ["1232878462404916650", "18148123035081125714"]}, "FL3ReD0AAAAJ:N5tVd3kTz84C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified Flow", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:N5tVd3kTz84C", "num_citations": 18, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6624595350082299901", "cites_id": ["6624595350082299901"]}, "FL3ReD0AAAAJ:b0M2c_1WBrUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Rethinking Evaluation Metrics of Open-Vocabulary Segmentation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:b0M2c_1WBrUC", "num_citations": 18, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=250972330721113447", "cites_id": ["250972330721113447"]}, "FL3ReD0AAAAJ:OU6Ihb5iCvQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A generalist facex via learning unified facial representation", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:OU6Ihb5iCvQC", "num_citations": 18, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1518916058955134215", "cites_id": ["1518916058955134215"]}, "FL3ReD0AAAAJ:86PQX7AUzd4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Llavadi: What matters for multimodal large language models distillation", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:86PQX7AUzd4C", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4205348834621157966", "cites_id": ["4205348834621157966"]}, "FL3ReD0AAAAJ:tOudhMTPpwUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Generalizable entity grounding via assistance of large language model", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:tOudhMTPpwUC", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3141318361462047746", "cites_id": ["3141318361462047746"]}, "FL3ReD0AAAAJ:K3LRdlH-MEoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:K3LRdlH-MEoC", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6961714241041144859", "cites_id": ["6961714241041144859"]}, "FL3ReD0AAAAJ:nb7KW1ujOQ8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Exploring self-supervised learning for multi-modal remote sensing pre-training via asymmetric attention fusion", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:nb7KW1ujOQ8C", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4623777589384587739,6300814273934637415", "cites_id": ["4623777589384587739", "6300814273934637415"]}, "FL3ReD0AAAAJ:FAceZFleit8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "OmniAudio: Generating Spatial Audio from 360-Degree Video", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:FAceZFleit8C", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4088694552939636976,10706180370784119878", "cites_id": ["4088694552939636976", "10706180370784119878"]}, "FL3ReD0AAAAJ:0EnyYjriUFMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Fast and Accurate Scene Parsing via Bi-direction Alignment Networks", "pub_year": "2021"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:0EnyYjriUFMC", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13200143004146598634,13242031612549546711", "cites_id": ["13200143004146598634", "13242031612549546711"]}, "FL3ReD0AAAAJ:BJbdYPG6LGMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:BJbdYPG6LGMC", "num_citations": 15, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8478320480881801966", "cites_id": ["8478320480881801966"]}, "FL3ReD0AAAAJ:b1wdh0AR-JQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:b1wdh0AR-JQC", "num_citations": 15, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8004158204501080342", "cites_id": ["8004158204501080342"]}, "FL3ReD0AAAAJ:_B80troHkn4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "VG4D: Vision-Language Model Goes 4D Video Recognition", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:_B80troHkn4C", "num_citations": 15, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7099338885326100787", "cites_id": ["7099338885326100787"]}, "FL3ReD0AAAAJ:HeT0ZceujKMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for motion modelling in radiotherapy: beyond classic voxel-based methods", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:HeT0ZceujKMC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11243233361944204569,14726571233642805715", "cites_id": ["11243233361944204569", "14726571233642805715"]}, "FL3ReD0AAAAJ:8k81kl-MbHgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Improving Video Instance Segmentation via Temporal Pyramid Routing", "pub_year": "2022"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:8k81kl-MbHgC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17281020540895090022", "cites_id": ["17281020540895090022"]}, "FL3ReD0AAAAJ:DUooU5lO8OsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mixed-r1: Unified reward perspective for reasoning capability in multimodal large language models", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:DUooU5lO8OsC", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7899069141862836072", "cites_id": ["7899069141862836072"]}, "FL3ReD0AAAAJ:i2xiXl-TujoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:i2xiXl-TujoC", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16225528582548364990", "cites_id": ["16225528582548364990"]}, "FL3ReD0AAAAJ:ML0RJ9NH7IQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "RelationBooth: Towards Relation-Aware Customized Object Generation", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:ML0RJ9NH7IQC", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8002148285439541657,4551599154503946899", "cites_id": ["8002148285439541657", "4551599154503946899"]}, "FL3ReD0AAAAJ:SeFeTyx0c_EC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Neural collapse terminus: A unified solution for class incremental learning and its variants", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:SeFeTyx0c_EC", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12030020674905768233,4417267262771970043", "cites_id": ["12030020674905768233", "4417267262771970043"]}, "FL3ReD0AAAAJ:pqnbT2bcN3wC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision", "pub_year": "2023"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:pqnbT2bcN3wC", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12262659112539402740,5137837445038889525", "cites_id": ["12262659112539402740", "5137837445038889525"]}, "FL3ReD0AAAAJ:UebtZRa9Y70C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Boundarysqueeze: Image segmentation as boundary squeezing", "pub_year": "2021"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:UebtZRa9Y70C", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=292756853765340773", "cites_id": ["292756853765340773"]}, "FL3ReD0AAAAJ:a9-T7VOCCH8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:a9-T7VOCCH8C", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16026523829622046068,12759230841376437442", "cites_id": ["16026523829622046068", "12759230841376437442"]}, "FL3ReD0AAAAJ:WbkHhVStYXYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ModelNet-O: A Large-Scale Synthetic Dataset for Occlusion-Aware Point Cloud Classification", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:WbkHhVStYXYC", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7602685551982351937", "cites_id": ["7602685551982351937"]}, "FL3ReD0AAAAJ:V3AGJWp-ZtQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:V3AGJWp-ZtQC", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10157668152982396045", "cites_id": ["10157668152982396045"]}, "FL3ReD0AAAAJ:LI9QrySNdTsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Are They the Same? Exploring Visual Correspondence Shortcomings of Multimodal LLMs", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:LI9QrySNdTsC", "num_citations": 10, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9461818450777280555", "cites_id": ["9461818450777280555"]}, "FL3ReD0AAAAJ:48xauSegjOkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "One flight over the gap: A survey from perspective to panoramic vision", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:48xauSegjOkC", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7714377988631594367,9049463542270942886", "cites_id": ["7714377988631594367", "9049463542270942886"]}, "FL3ReD0AAAAJ:uDGL6kOW6j0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:uDGL6kOW6j0C", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8717469655529452608,941170495020319536", "cites_id": ["8717469655529452608", "941170495020319536"]}, "FL3ReD0AAAAJ:IUKN3-7HHlwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:IUKN3-7HHlwC", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15186092869329584942", "cites_id": ["15186092869329584942"]}, "FL3ReD0AAAAJ:PVgj2kMGcgYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:PVgj2kMGcgYC", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8886195247312812296,5642270768482413262,5324036314113346793", "cites_id": ["8886195247312812296", "5642270768482413262", "5324036314113346793"]}, "FL3ReD0AAAAJ:4hFrxpcac9AC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:4hFrxpcac9AC", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8916259866515024855", "cites_id": ["8916259866515024855"]}, "FL3ReD0AAAAJ:_axFR9aDTf0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:_axFR9aDTf0C", "num_citations": 6, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2868625499116191367", "cites_id": ["2868625499116191367"]}, "FL3ReD0AAAAJ:5qfkUJPXOUwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Unified Dense Prediction of Video Diffusion", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:5qfkUJPXOUwC", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12310180994894236257", "cites_id": ["12310180994894236257"]}, "FL3ReD0AAAAJ:_FxGoFyzp5QC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Torchcv: A pytorch-based framework for deep learning in computer vision", "pub_year": "2019"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:_FxGoFyzp5QC", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9786951919037805436", "cites_id": ["9786951919037805436"]}, "FL3ReD0AAAAJ:QD3KBmkZPeQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Visual spatial tuning", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:QD3KBmkZPeQC", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9469345150003225573", "cites_id": ["9469345150003225573"]}, "FL3ReD0AAAAJ:yB1At4FlUx8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "You Can't Ignore Either: Unifying Structure and Feature Denoising for Robust Graph Learning", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:yB1At4FlUx8C", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4389577798254339597", "cites_id": ["4389577798254339597"]}, "FL3ReD0AAAAJ:nRpfm8aw39MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Vision-language-action models for autonomous driving: Past, present, and future", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:nRpfm8aw39MC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7235377509585290317", "cites_id": ["7235377509585290317"]}, "FL3ReD0AAAAJ:1taIhTC69MYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "EMOv2: Pushing 5 M Vision Model Frontier", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:1taIhTC69MYC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15262123136209008253", "cites_id": ["15262123136209008253"]}, "FL3ReD0AAAAJ:yqoGN6RLRZoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Synergistic Dual Spatial-aware Generation of Image-to-text and Text-to-image", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:yqoGN6RLRZoC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6360953540944997921", "cites_id": ["6360953540944997921"]}, "FL3ReD0AAAAJ:zLWjf1WUPmwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Auto cherry-picker: Learning from high-quality generative data driven by language", "pub_year": "2024"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:zLWjf1WUPmwC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8326793408226204248", "cites_id": ["8326793408226204248"]}, "FL3ReD0AAAAJ:jL-93Qbq4QoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation", "pub_year": "2026"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:jL-93Qbq4QoC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13819143505778771184", "cites_id": ["13819143505778771184"]}, "FL3ReD0AAAAJ:Aul-kAQHnToC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "pub_year": "2025"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:Aul-kAQHnToC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=18327982909852583551", "cites_id": ["18327982909852583551"]}, "FL3ReD0AAAAJ:WHdLCjDvYFkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SAMTok: Representing Any Mask with Two Words", "pub_year": "2026"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:WHdLCjDvYFkC", "num_citations": 0}, "FL3ReD0AAAAJ:0CzhzZyukY4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs", "pub_year": "2026"}, "filled": false, "author_pub_id": "FL3ReD0AAAAJ:0CzhzZyukY4C", "num_citations": 0}}, "citedby5y": 8957, "hindex": 47, "hindex5y": 47, "i10index": 101, "i10index5y": 101, "cites_per_year": {"2020": 77, "2021": 266, "2022": 546, "2023": 1042, "2024": 2349, "2025": 4490, "2026": 251}, "updated": "2026-01-29 08:33:04.500306"}