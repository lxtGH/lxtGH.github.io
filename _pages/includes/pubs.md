# üìù Publications  

\* means equal contribution.


## Recent Works 

<li><a href="https://arxiv.org/abs/2504.10465">Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding</a>,  
    Tao Zhang, Xiangtai Li, Zilong Huang, Yanwei Li, Weixian Lei, Xueqing Deng, Shihao Chen, Shunping Ji, Jiashi Feng
      <strong><span style="color:red"> The simplest architecture for pixel-grounding tasks. </span> </strong> | <a href="https://github.com/magic-research/Sa2VA">Code</a> </li>

<li><a href="https://arxiv.org/abs/2504.10462">The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer</a>,  
     Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, Zilong Huang
      <strong>ICCv 2025<span style="color:red"> Scaling Single Transformer backbone training for both VLM and vision tasks. </span> </strong> | <a href="https://github.com/bytedance/SAIL">Code</a> </li>

<li><a href="https://arxiv.org/abs/2501.04001">Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos</a>,  
     Haobo Yuan, <strong>Xiangtai Li</strong>, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, Ming-Hsuan Yang
      <strong><span style="color:red"> Marrying SAM2 with LLaVA-like MLLM for open-ended spatial temporal understanding. </span> </strong> | <a href="https://lxtgh.github.io/project/sa2va/">Project Page</a> </li>

<li><a href="https://arxiv.org/abs/2501.04670">Are They the Same? Exploring Visual Correspondence Shortcomings of Multimodal LLMs</a>,  
    Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, <strong>Xiangtai Li</strong>, Lu Qi
      <strong>ICCV 2025<span style="color:red"> The first MLLM visual matching benchmark and a simple contrastive token solution. </span> </strong> | <a href="https://zhouyiks.github.io/projects/CoLVA/">Project Page</a> </li>


## Several Other Recent Works

<li><a href="https://arxiv.org/abs/2410.23280">DreamRelation: Bridging Customization and Relation Generation</a>,
    Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, <strong>Xiangtai Li</strong>
      <strong>CVPR 2025<span style="color:red"> (oral) First relation-aware customization approach </span> </strong> | <a href="https://shi-qingyu.github.io/DreamRelation">Github</a> </li>

<li><a href="https://arxiv.org/abs/2412.07589">DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation</a>,  
     Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, <strong>Xiangtai Li</strong>, Yunhai Tong
      <strong>CVPR 2025 <span style="color:red"> The first MLLM-based generation method for customized manga generation. </span> </strong> | <a href="https://jianzongwu.github.io/projects/diffsensei/">Project Page</a> </li>

<li><a href="https://arxiv.org/abs/2410.08261">Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis</a>,
    Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen,  <strong>Xiangtai Li</strong>, Zhen Dong, Lei Zhu, Shuicheng Yan
      <strong>ICLR 2025<span style="color:red"> Make Masked Generative Transformer For Text to Image Generation Great Again! </span> </strong> | <a href="https://github.com/viiika/Meissonic">Github</a> </li>

<li><a href="https://arxiv.org/abs/2406.05127">Towards Semantic Equivalence of Tokenization in Multimodal LLM</a>,
    Shengqiong Wu, Hao Fei, <strong>Xiangtai Li</strong>, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, Shuicheng Yan
      <strong>ICLR 2025<span style="color:red"> A new visual tokenizer for various MLLMs design. </span> </strong> | <a href="https://sqwu.top/SeTok-web/">Github</a> </li>

<li><a href="https://arxiv.org/abs/2401.10228">RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything</a>,
   Shilin Xu, Haobo Yuan, Qingyu Shi, Lu Qi, Jingbo Wang, Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard Ghanem,  <strong>Xiangtai Li</strong>, Ming-Hsuan Yang
      <strong>ICLR 2025<span style="color:red"> (oral) A new real-time multi-task segmentation setting, benchmark, and a simple effcient baseline. </span> </strong> | <a href="https://github.com/xushilin1/RAP-SAM/">Github</a> </li>


<li><a href="https://arxiv.org/abs/2401.10229">OMG-Seg: Is One Model Good Enough For All Segmentation?</a>,  
     <strong>Xiangtai Li</strong>, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy
      <strong>CVPR 2024 <span style="color:red"> One model to perform image/video/open-vocabulary/multi-dataset/interactive segmentation in one shot. </span> </strong> | <a href=" https://lxtgh.github.io/project/omg_seg/">Project Page</a> </li>

<li><a href="https://arxiv.org/abs/2406.19389">OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding</a>,  
     Tao Zhang, <strong>Xiangtai Li</strong>, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, Shuicheng Yan
      <strong>NeurIPS 2024 <span style="color:red"> The first end-to-end MLLM that unifies image-level, object-level, pixel-level understanding and reasoning. </span> </strong> | <a href="https://github.com/lxtGH/OMG-Seg">Github</a> </li>

<li><a href="https://arxiv.org/abs/2406.17758">MotionBooth: Motion-Aware Customized Text-to-Video Generation</a>,  
     Jianzong Wu, <strong>Xiangtai Li</strong>, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, Kai Chen
      <strong>NeurIPS 2024 (spotlight)<span style="color:red"> The first customized T2V with motion control. </span> </strong> | <a href="https://github.com/jianzongwu/MotionBooth">Github</a> </li>


## Several Previous Works

<li><a href="https://arxiv.org/abs/2304.09854">Transformer-Based Visual Segmentation: A Survey</a>,  
     <strong>Xiangtai Li</strong>, Henghui Ding, Haobo Yuan, Wenwei Zhang, Jiangmiao Pang, Guangliang Cheng, Kai Chen, Ziwei Liu, Chen Change Loy
      <strong>T-PAMI 2024 <span style="color:red"> The first survey that summarizes the transformer-based segmentation method from technical views. (PAMI popular paper) </span> </strong> | <a href="https://github.com/lxtGH/Awesome-Segmentation-With-Transformer">Github</a> </li>

<li><a href="https://arxiv.org/abs/2303.12782">Tube-Link: A Flexible Cross Tube Baseline for Universal Video Segmentation</a>,  
      <strong>Xiangtai Li</strong>, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, Chen Change Loy,
      <strong>ICCV 2023 <span style="color:red"> The first unified SOTA universal video segmentation model. </span> </strong> | <a href="https://github.com/lxtGH/Tube-Link">Project</a> </li>

<li><a href="https://arxiv.org/abs/2201.05047"> TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers </a>,  
    Qianyu Zhou*,  <strong> Xiangtai Li* </strong>, Lu He, Yibo Yang, Guangliang Cheng, Yunhai Tong, Lizhuang Ma, Dacheng Tao,
      <strong>T-PAMI 2023 <span style="color:red"> The first End-to-End Vision Transformer for Video Object Detection and STOA results on Video Object Detection </span> </strong> | <a href="https://github.com/SJTU-LuHe/TransVOD">Code</a> </li>

<li><a href="https://arxiv.org/abs/2204.04656">Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation</a>,  
      <strong>Xiangtai Li*</strong>, Wenwei Zhang*, Jiangmiao Pang*, Kai Chen, Guangliang Cheng, Yunhai Tong, Chen Change Loy,
      <strong>CVPR 2022 <span style="color:red">(Oral, top2%) The first unified video segmentation model and codebase for VPS, VIS, VSS</span> </strong> | <a href="https://github.com/lxtGH/Video-K-Net">Code</a> </li>

<li><a href="https://arxiv.org/abs/2002.10120">Semantic Flow for Fast and Accurate Scene Parsing</a>,  
      <strong>Xiangtai Li</strong>, Ansheng You, Zhen Zhu, Houlong Zhao, Maoke Yang, Kuiyuan Yang, Yunhai Tong,
      <strong>ECCV 2020 <span style="color:red">(Oral, top2%) The first real-time model over 80% mIoU on Cityscapes test set.</span></strong> | <a href="https://github.com/lxtGH/SFSegNets">Code</a> </li>

Code can be found in [this](https://github.com/lxtGH).