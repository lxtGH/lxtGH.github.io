# ðŸ”¥ News

[//]: # (- *2024.04*ï¼š ðŸ”¥ðŸ”¥ Checkout our Mamba works. [Point Cloud Mamba]&#40;https://arxiv.org/abs/2403.00762&#41;, [MambaAD]&#40;https://arxiv.org/abs/2404.06564&#41;, and [DGMamba]&#40;https://arxiv.org/abs/2404.07794&#41;. )

- *2025.01*   &nbsp; ðŸ”¥ðŸ”¥ Checkout our recent works on video MLLM, [Sa2VA](https://arxiv.org/abs/2501.04001), combine both SAM-2 and LLaVA in one-shot.
- *2024.12*   &nbsp; ðŸ”¥ðŸ”¥ Serving as an Area Chair for both ICML-2025 and ICCV-2025!
- *2024.12*   &nbsp;ðŸŽ‰ðŸŽ‰ Several works on AAAI-2025 and 3DV-2025. [Point Cloud Mamba](https://arxiv.org/abs/2403.00762), [Point RWKV](https://arxiv.org/abs/2405.15214), [LDM-Seg](https://arxiv.org/abs/2403.09616), [ReasonSeg3D](https://arxiv.org/abs/2405.17427).
- *2024.09*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Several works on NeurIPS-2024. [OMG-LLaVA](https://arxiv.org/abs/2406.19389), [MotionBooth](https://arxiv.org/abs/2406.17758) (<span style="color:red">spotlight</span>), [SemFlow](https://arxiv.org/abs/2405.20282), [MamabaAD](https://arxiv.org/abs/2404.06564). Thanks for all co-authors' help!
- *2024.07*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Our Transformer Survey is finally accepted by T-PAMI. [Arxiv](https://arxiv.org/abs/2304.09854).
- *2024.07*:  ðŸ”¥ðŸ”¥ The training code of Edge-SAM and corresponding app, "Cutcha" in IOS shop, are available now, [link](https://apps.apple.com/us/app/cutcha-photo/id6478521132). [Code](https://github.com/chongzhou96/EdgeSAM).
- *2024.07*ï¼š ðŸ”¥ðŸ”¥ Checkout our recent Universal Dense MLLM Model, OMG-LLaVA, [project](https://lxtgh.github.io/project/omg_llava/), [code](https://github.com/lxtGH/OMG-Seg/tree/main/omg_llava).
- *2024.07*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ [DVIS-DAQ](https://zhang-tao-whu.github.io/projects/DVIS_DAQ/), [Open-Vocabulary SAM](https://www.mmlab-ntu.com/project/ovsam/), [FaceAdapter](https://github.com/FaceAdapter/Face-Adapter), and [GenView](https://arxiv.org/pdf/2403.12003) are accepted by ECCV-2024. All code and models are released.
- *2024.06*ï¼š ðŸ”¥ðŸ”¥ Checkout our recent works on MLLM and new architecture design, [OMG-LLaVA](https://arxiv.org/abs/2406.19389), [RWKV-SAM](https://arxiv.org/abs/2406.19369), [MotionBooth](https://arxiv.org/abs/2406.17758), [SeTok](https://arxiv.org/abs/2406.05127) and [Reason3D](https://arxiv.org/abs/2405.17427).

[//]: # (- *2024.06*ï¼š ðŸ”¥ðŸ”¥ Checkout our recent works on diffusion models, [MotionBooth]&#40;https://arxiv.org/abs/2406.17758&#41;, [SemFlow]&#40;&#41;.)

[//]: # (- *2024.04*ï¼š ðŸ”¥ðŸ”¥ Checkout our new video segmentation work [DVIS-DAQ]&#40;https://arxiv.org/abs/2404.00086&#41;, which achieves the new state-of-the-art results on multiple video segmentation benchmarks.)

[//]: # (- *2024.04*ï¼š ðŸ”¥ðŸ”¥ Checkout [Point Cloud Mamba]&#40;https://arxiv.org/abs/2403.00762&#41;, the first SSMs-model that performs better than PointMLP and PointTransformer!)

[//]: # (- *2024.03*ï¼š ðŸ”¥ðŸ”¥ The codebase of OMG-Seg is open-sourced! [link]&#40;https://github.com/lxtGH/OMG-Seg&#41;. This is the first codebase support joint image/video/multi-data/interactive segmentation co-training and testing!)

[//]: # (- *2024.03*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Give a talk of open-world segmentation &#40;Beyond SAM&#41; at [VALSE]&#40;http://valser.org/&#41;, [Slides]&#40;../../project/paper_local/xiangtai_valse_talk_3_20_2024.pdf&#41; [Video]&#40;https://www.bilibili.com/video/BV1PZ421b7U7/?spm_id_from=333.337.search-card.all.click&vd_source=6bb672e5bcff6f43a998d1ba30743967&#41;.)

[//]: # (- *2024.02*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ OMG-Seg is accepted by CVPR-24. Along with OMG-Seg, five works are accepted by CVPR-24! [BA-SAM]&#40;https://arxiv.org/abs/2401.02317&#41;, [RTMO]&#40;https://arxiv.org/abs/2312.07526&#41;, [Skeleton-in-Context]&#40;https://arxiv.org/abs/2312.03703&#41;, and [language-driven video inpainting]&#40;https://arxiv.org/abs/2401.10226&#41;. )

[//]: # (- *2024.02*: Checkout several recent works on segmentation and recognition, [OMG-Seg]&#40;https://arxiv.org/abs/2401.10229&#41;, [Open-Vocabulary SAM]&#40;https://arxiv.org/abs/2401.02955&#41; and [RAP-SAM]&#40;https://arxiv.org/abs/2401.10228&#41;.)

[//]: # (- *2024.01*: &nbsp;ðŸŽ‰ðŸŽ‰ <a href="https://arxiv.org/abs/2306.15880"> Our survey </a> on Open Vocabulary Learning is accepted by T-PAMI.)

[//]: # (- *2023.12*: Checkout [EdgeSAM]&#40;https://arxiv.org/abs/2312.06660&#41;, a mobile SAM that can run on iPhone! )

[//]: # (- *2023.10*: Checkout our recent works on Open-Vocabulary Detection and Segmentation. [DST-Det]&#40;https://arxiv.org/abs/2310.01393&#41;, [CLIPSelf]&#40;https://arxiv.org/abs/2310.01403&#41;, [MosaicFusion]&#40;https://arxiv.org/abs/2309.13042&#41;.)

[//]: # (- *2023.09*: &nbsp;ðŸŽ‰ðŸŽ‰ Two NeurIPS Paper are accepted as SpotLight. PSG4D and Point-In-Context.)
[//]: # (- *2023.08*: Give a talk of video segmentation at [VALSE]&#40;http://valser.org/&#41; and [Slides]&#40;../../project/paper_local/talk-valse-8-30-2023.pdf&#41;.)

[//]: # (- *2023.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Three paper in ICCV-23: Tube-Link, Betrayed Caption and EMO-Net. One Oral Paper in ICCV-23 workshop. See you in Paris!!  SFNet-Lite is accepted by IJCV.)

[//]: # (- *2023.06*: Checkout our <a href="https://arxiv.org/abs/2306.08659"> new paper </a> on point cloud in-context learning and <a href="https://arxiv.org/abs/2306.15880"> the first survey</a>  on Open Vocabulary Learning. )

[//]: # (- *2023.03*: Checkout our <a href="https://arxiv.org/abs/2304.09854"> new survey </a> on transformer-based segmentation and detection, Also Video Talk, Chinese, [Link]&#40;https://www.bilibili.com/video/BV1tc411M7DC/?spm_id_from=333.337.search-card.all.click&vd_source=6bb672e5bcff6f43a998d1ba30743967&#41;. )

[//]: # (- *2023.03*ï¼šPlease checkout our new work, <a href="https://arxiv.org/abs/2303.12782">Tube-Link</a>, the first universal video segmentation framework that outperforms specific video segmentation methods &#40;VIS,VSS,VPS&#41;.)

[//]: # (- *2023.03*ï¼šOne paper on Panoptic Video Scene Graph Generation &#40;PVSG&#41; is accepted by CVPR-2023.)

[//]: # (- *2022.11*ï¼šTwo paper on Video Scene Understanding is accepted by T-PAMI.)

[//]: # (- *2022.09*ï¼šOne paper on Neural Collapse is accepted by NeurIPS-2022. )

[//]: # (- *2022.08*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Join the MMLab@NTU S-Lab! Our four works &#40;Video K-Net, PanopticPartFormer, FashionFormer, and PolyphonicFormer in CVPR-22/ECCV-22&#41; code are all released. Check out my github homepage.)

[//]: # (- *2022.07*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Our SFNet-Lite &#40;extension of SFNet-ECCV20&#41; achieve the best mIoU and speed trade-off.)

[//]: # (on multiple driving datasets. SFNet-Lite can obtain 80.1 mIoU while running at 50 FPS, 78.8 mIoU while running at 120 FPS. [Code]&#40;https://github.com/lxtGH/SFSegNets&#41;.)

[//]: # (- *2022.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Three papers are accepted by ECCV-2022. One paper is accepted by ICIP-2022.)

[//]: # (- *2022.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Graduated From PKU. )

[//]: # (- *2022.03*: &nbsp;ðŸŽ‰ðŸŽ‰ Video K-Net is accepted by CVPR-2022 as oral presentation.  )
