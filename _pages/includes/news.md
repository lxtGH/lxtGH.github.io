# ðŸ”¥ News

- *2023.12*: Checkout [EdgeSAM](https://arxiv.org/abs/2312.06660), a mobile SAM that can run on iPhone! 
- *2023.10*: Checkout our recent works on Open-Vocabulary Detection and Segmentation. [DST-Det](https://arxiv.org/abs/2310.01393), [CLIPSelf](https://arxiv.org/abs/2310.01403), [MosaicFusion](https://arxiv.org/abs/2309.13042).
- *2023.09*: &nbsp;ðŸŽ‰ðŸŽ‰ Two NeurIPS Paper are accepted as SpotLight. PSG4D and Point-In-Context.
- *2023.08*: Give a talk of video segmentation at [Valse](http://valser.org/) and [Slides](../../project/paper_local/talk-valse-8-30-2023.pdf).
- *2023.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Three paper in ICCV-23: Tube-Link, Betrayed Caption and EMO-Net. One Oral Paper in ICCV-23 workshop. See you in Paris!!  SFNet-Lite is accepted by IJCV.
- *2023.06*: Checkout our <a href="https://arxiv.org/abs/2306.08659"> new paper </a> on point cloud in-context learning and <a href="https://arxiv.org/abs/2306.15880"> the first survey</a>  on Open Vocabulary Learning. 
- *2023.03*: Checkout our <a href="https://arxiv.org/abs/2304.09854"> new survey </a> on transformer-based segmentation and detection, Also Video Talk, Chinese, [Link](https://www.bilibili.com/video/BV1tc411M7DC/?spm_id_from=333.337.search-card.all.click&vd_source=6bb672e5bcff6f43a998d1ba30743967). 

[//]: # (- *2023.03*ï¼šPlease checkout our new work, <a href="https://arxiv.org/abs/2303.12782">Tube-Link</a>, the first universal video segmentation framework that outperforms specific video segmentation methods &#40;VIS,VSS,VPS&#41;.)

[//]: # (- *2023.03*ï¼šOne paper on Panoptic Video Scene Graph Generation &#40;PVSG&#41; is accepted by CVPR-2023.)

[//]: # (- *2022.11*ï¼šTwo paper on Video Scene Understanding is accepted by T-PAMI.)

[//]: # (- *2022.09*ï¼šOne paper on Neural Collapse is accepted by NeurIPS-2022. )

[//]: # (- *2022.08*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Join the MMLab@NTU S-Lab! Our four works &#40;Video K-Net, PanopticPartFormer, FashionFormer, and PolyphonicFormer in CVPR-22/ECCV-22&#41; code are all released. Check out my github homepage.)

[//]: # (- *2022.07*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Our SFNet-Lite &#40;extension of SFNet-ECCV20&#41; achieve the best mIoU and speed trade-off.)

[//]: # (on multiple driving datasets. SFNet-Lite can obtain 80.1 mIoU while running at 50 FPS, 78.8 mIoU while running at 120 FPS. [Code]&#40;https://github.com/lxtGH/SFSegNets&#41;.)

[//]: # (- *2022.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Three papers are accepted by ECCV-2022. One paper is accepted by ICIP-2022.)

[//]: # (- *2022.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Graduated From PKU. )

[//]: # (- *2022.03*: &nbsp;ðŸŽ‰ðŸŽ‰ Video K-Net is accepted by CVPR-2022 as oral presentation.  )
