# ðŸ”¥ News
- *2023.07*: Three paper in ICCV-23: Tube-Link, Betrayed Caption and EMO-Net. One Paper in ICCV-23 workshop. See you in Paris!!  SFNet-Lite is accepted by IJCV.
- *2023.06*: Checkout our <a href="https://arxiv.org/abs/2306.08659"> new paper </a> on point cloud in-context learning and <a href="https://arxiv.org/abs/2306.15880"> the first survey</a>  on Open Vocabulary Learning. 
- *2023.03*: Checkout our <a href="https://arxiv.org/abs/2304.09854"> new survey </a> on transformer-based segmentation and detection. 
- *2023.03*ï¼šPlease checkout our new work, <a href="https://arxiv.org/abs/2303.12782">Tube-Link</a>, the first universal video segmentation framework that outperforms specific video segmentation methods (VIS,VSS,VPS).
- *2023.03*ï¼šOne paper on Panoptic Video Scene Graph Generation (PVSG) is accepted by CVPR-2023.
- *2022.11*ï¼šTwo paper on Video Scene Understanding is accepted by T-PAMI.
- *2022.09*ï¼šOne paper on Neural Collapse is accepted by NeurIPS-2022. 
- *2022.08*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Join the MMLab@NTU S-Lab! Our four works (Video K-Net, PanopticPartFormer, FashionFormer, and PolyphonicFormer in CVPR-22/ECCV-22) code are all released. Check out my github homepage.
- *2022.07*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Our SFNet-Lite (extension of SFNet-ECCV20) achieve the best mIoU and speed trade-off.
on multiple driving datasets. SFNet-Lite can obtain 80.1 mIoU while running at 50 FPS, 78.8 mIoU while running at 120 FPS. [Code](https://github.com/lxtGH/SFSegNets).
- *2022.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Three papers are accepted by ECCV-2022. One paper is accepted by ICIP-2022.
- *2022.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Graduated From PKU. 
- *2022.03*: &nbsp;ðŸŽ‰ðŸŽ‰ Video K-Net is accepted by CVPR-2022 as oral presentation.  
