# ðŸ”¥ News
- *2024.04*ï¼š ðŸ”¥ðŸ”¥ Checkout our Mamba works. [Point Cloud Mamba](https://arxiv.org/abs/2403.00762), [MambaAD](https://arxiv.org/abs/2404.06564), and [DGMamba](https://arxiv.org/abs/2404.07794). 
- *2024.04*ï¼š ðŸ”¥ðŸ”¥ Checkout our open-sourced codebase [ADer](https://github.com/zhangzjn/ader) for the state-of-the-art anomaly detection AD methods.
- *2024.04*ï¼š ðŸ”¥ðŸ”¥ Checkout our new video segmentation work [DVIS-DAQ](https://arxiv.org/abs/2404.00086), which achieves the new state-of-the-art results on multiple video segmentation benchmark.
- *2024.03*ï¼š ðŸ”¥ðŸ”¥ The codebase of OMG-Seg is open-sourced! [link](https://github.com/lxtGH/OMG-Seg). This is the first codebase support joint image/video/multi-data/interactive segmentation co-training and testing!
- *2024.03*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Give a talk of open-world segmentation (Beyond SAM) at [VALSE](http://valser.org/), [Slides](../../project/paper_local/xiangtai_valse_talk_3_20_2024.pdf) [Video](https://www.bilibili.com/video/BV1PZ421b7U7/?spm_id_from=333.337.search-card.all.click&vd_source=6bb672e5bcff6f43a998d1ba30743967).
- *2024.02*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ OMG-Seg is accepted by CVPR-24. Along with OMG-Seg, five works are accepted by CVPR-24! [BA-SAM](https://arxiv.org/abs/2401.02317), [RTMO](https://arxiv.org/abs/2312.07526), [Skeleton-in-Context](https://arxiv.org/abs/2312.03703), and [language-driven video inpainting](https://arxiv.org/abs/2401.10226). 
- *2024.02*: Checkout several recent works on segmentation and recognition, [OMG-Seg](https://arxiv.org/abs/2401.10229), [Open-Vocabulary SAM](https://arxiv.org/abs/2401.02955) and [RAP-SAM](https://arxiv.org/abs/2401.10228).
- *2024.01*: &nbsp;ðŸŽ‰ðŸŽ‰ <a href="https://arxiv.org/abs/2306.15880"> Our survey </a> on Open Vocabulary Learning is accepted by T-PAMI.
- *2023.12*: Checkout [EdgeSAM](https://arxiv.org/abs/2312.06660), a mobile SAM that can run on iPhone! 
- *2023.10*: Checkout our recent works on Open-Vocabulary Detection and Segmentation. [DST-Det](https://arxiv.org/abs/2310.01393), [CLIPSelf](https://arxiv.org/abs/2310.01403), [MosaicFusion](https://arxiv.org/abs/2309.13042).

[//]: # (- *2023.09*: &nbsp;ðŸŽ‰ðŸŽ‰ Two NeurIPS Paper are accepted as SpotLight. PSG4D and Point-In-Context.)
[//]: # (- *2023.08*: Give a talk of video segmentation at [VALSE]&#40;http://valser.org/&#41; and [Slides]&#40;../../project/paper_local/talk-valse-8-30-2023.pdf&#41;.)

[//]: # (- *2023.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Three paper in ICCV-23: Tube-Link, Betrayed Caption and EMO-Net. One Oral Paper in ICCV-23 workshop. See you in Paris!!  SFNet-Lite is accepted by IJCV.)

[//]: # (- *2023.06*: Checkout our <a href="https://arxiv.org/abs/2306.08659"> new paper </a> on point cloud in-context learning and <a href="https://arxiv.org/abs/2306.15880"> the first survey</a>  on Open Vocabulary Learning. )

[//]: # (- *2023.03*: Checkout our <a href="https://arxiv.org/abs/2304.09854"> new survey </a> on transformer-based segmentation and detection, Also Video Talk, Chinese, [Link]&#40;https://www.bilibili.com/video/BV1tc411M7DC/?spm_id_from=333.337.search-card.all.click&vd_source=6bb672e5bcff6f43a998d1ba30743967&#41;. )

[//]: # (- *2023.03*ï¼šPlease checkout our new work, <a href="https://arxiv.org/abs/2303.12782">Tube-Link</a>, the first universal video segmentation framework that outperforms specific video segmentation methods &#40;VIS,VSS,VPS&#41;.)

[//]: # (- *2023.03*ï¼šOne paper on Panoptic Video Scene Graph Generation &#40;PVSG&#41; is accepted by CVPR-2023.)

[//]: # (- *2022.11*ï¼šTwo paper on Video Scene Understanding is accepted by T-PAMI.)

[//]: # (- *2022.09*ï¼šOne paper on Neural Collapse is accepted by NeurIPS-2022. )

[//]: # (- *2022.08*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Join the MMLab@NTU S-Lab! Our four works &#40;Video K-Net, PanopticPartFormer, FashionFormer, and PolyphonicFormer in CVPR-22/ECCV-22&#41; code are all released. Check out my github homepage.)

[//]: # (- *2022.07*ï¼š &nbsp;ðŸŽ‰ðŸŽ‰ Our SFNet-Lite &#40;extension of SFNet-ECCV20&#41; achieve the best mIoU and speed trade-off.)

[//]: # (on multiple driving datasets. SFNet-Lite can obtain 80.1 mIoU while running at 50 FPS, 78.8 mIoU while running at 120 FPS. [Code]&#40;https://github.com/lxtGH/SFSegNets&#41;.)

[//]: # (- *2022.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Three papers are accepted by ECCV-2022. One paper is accepted by ICIP-2022.)

[//]: # (- *2022.07*: &nbsp;ðŸŽ‰ðŸŽ‰ Graduated From PKU. )

[//]: # (- *2022.03*: &nbsp;ðŸŽ‰ðŸŽ‰ Video K-Net is accepted by CVPR-2022 as oral presentation.  )
